{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ffsy1cKaKA4M"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sl1qQw4392Ip",
        "outputId": "51e86c87-3b39-4e8a-d27b-c7718291758f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.24.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchdata in /usr/local/lib/python3.7/dist-packages (0.5.0)\n",
            "Requirement already satisfied: torch==1.13.0 in /usr/local/lib/python3.7/dist-packages (from torchdata) (1.13.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchdata) (2.23.0)\n",
            "Requirement already satisfied: portalocker>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from torchdata) (2.6.0)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.7/dist-packages (from torchdata) (1.25.11)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.13.0->torchdata) (4.1.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.7/dist-packages (from torch==1.13.0->torchdata) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.7/dist-packages (from torch==1.13.0->torchdata) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.7/dist-packages (from torch==1.13.0->torchdata) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.7/dist-packages (from torch==1.13.0->torchdata) (11.10.3.66)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->torchdata) (57.4.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->torchdata) (0.38.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (3.0.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.7/dist-packages (0.14.0)\n",
            "Requirement already satisfied: torch==1.13.0 in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.13.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext) (4.64.1)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.7/dist-packages (from torch==1.13.0->torchtext) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.7/dist-packages (from torch==1.13.0->torchtext) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.7/dist-packages (from torch==1.13.0->torchtext) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.7/dist-packages (from torch==1.13.0->torchtext) (11.7.99)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.13.0->torchtext) (4.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->torchtext) (57.4.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.0->torchtext) (0.38.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2.10)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install torchdata\n",
        "!pip install -U torchtext"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data.dataset import ConcatDataset\n",
        "from torchtext.datasets import RTE,MRPC\n",
        "torch.manual_seed(2022)\n",
        "random.seed(2022)"
      ],
      "metadata": {
        "id": "XZdKa7Mz9_uB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import CrossEntropyLoss, MSELoss\n",
        "from transformers import BertPreTrainedModel, BertModel,BertConfig\n",
        "class BertForSequenceClassification(BertPreTrainedModel):\n",
        "    def __init__(self,config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = 2\n",
        "        #config = BertConfig()\n",
        "        self.bert = BertModel(config)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.deterministic = False\n",
        "        self.ib_dim = 384\n",
        "        self.ib = True\n",
        "        #self.deterministic = True\n",
        "        self.activation = 'relu'\n",
        "        self.activations = {'tanh': nn.Tanh(), 'relu': nn.ReLU(), 'sigmoid': nn.Sigmoid()}\n",
        "        if self.ib or self.deterministic:\n",
        "            self.kl_annealing = \"linear\"\n",
        "            self.hidden_dim = (768 + self.ib_dim) // 2\n",
        "            intermediate_dim = (self.hidden_dim+768)//2\n",
        "            self.mlp_rte = nn.Sequential(\n",
        "                nn.Linear(768, intermediate_dim), #768\n",
        "                self.activations[self.activation],\n",
        "                nn.Linear(intermediate_dim, self.hidden_dim),\n",
        "                self.activations[self.activation])\n",
        "            self.mlp_mrpc = nn.Sequential(\n",
        "                nn.Linear(768, intermediate_dim), #768\n",
        "                self.activations[self.activation],\n",
        "                nn.Linear(intermediate_dim, self.hidden_dim),\n",
        "                self.activations[self.activation])\n",
        "            self.beta = 1e-03\n",
        "            self.sample_size = 10 #5 \n",
        "            self.emb2mu_rte = nn.Linear(self.hidden_dim, self.ib_dim)\n",
        "            self.emb2std_rte = nn.Linear(self.hidden_dim, self.ib_dim)\n",
        "            self.mu_p_rte = nn.Parameter(torch.randn(self.ib_dim))\n",
        "            self.std_p_rte = nn.Parameter(torch.randn(self.ib_dim))\n",
        "            self.emb2mu_mrpc = nn.Linear(self.hidden_dim, self.ib_dim)\n",
        "            self.emb2std_mrpc = nn.Linear(self.hidden_dim, self.ib_dim)\n",
        "            self.mu_p_mrpc = nn.Parameter(torch.randn(self.ib_dim))\n",
        "            self.std_p_mrpc = nn.Parameter(torch.randn(self.ib_dim))\n",
        "            self.classifier_rte = nn.Linear(self.ib_dim, self.num_labels)\n",
        "            self.classifier_mrpc = nn.Linear(self.ib_dim, self.num_labels) \n",
        "        else:\n",
        "            self.classifier = nn.Linear(768, self.num_labels)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def estimate(self, emb, emb2mu, emb2std):\n",
        "        \"\"\"Estimates mu and std from the given input embeddings.\"\"\"\n",
        "        mean = emb2mu(emb)\n",
        "        std = torch.nn.functional.softplus(emb2std(emb))\n",
        "        return mean, std\n",
        "\n",
        "    def kl_div(self, mu_q, std_q, mu_p, std_p):\n",
        "        \"\"\"Computes the KL divergence between the two given variational distribution.\\\n",
        "           This computes KL(q||p), which is not symmetric. It quantifies how far is\\\n",
        "           The estimated distribution q from the true distribution of p.\"\"\"\n",
        "        k = mu_q.size(1)\n",
        "        mu_diff = mu_p - mu_q\n",
        "        mu_diff_sq = torch.mul(mu_diff, mu_diff)\n",
        "        logdet_std_q = torch.sum(2 * torch.log(torch.clamp(std_q, min=1e-8)), dim=1)\n",
        "        logdet_std_p = torch.sum(2 * torch.log(torch.clamp(std_p, min=1e-8)), dim=1)\n",
        "        fs = torch.sum(torch.div(std_q ** 2, std_p ** 2), dim=1) + torch.sum(torch.div(mu_diff_sq, std_p ** 2), dim=1)\n",
        "        kl_divergence = (fs - k + logdet_std_p - logdet_std_q)*0.5\n",
        "        return kl_divergence.mean()\n",
        "\n",
        "    def reparameterize(self, mu, std):\n",
        "        batch_size = mu.shape[0]\n",
        "        z = torch.randn(self.sample_size, batch_size, mu.shape[1]).cuda()\n",
        "        return mu + std * z\n",
        "\n",
        "    def get_logits(self, z, mu, sampling_type,dataset_name):\n",
        "        if sampling_type == \"iid\":\n",
        "            if dataset_name == \"rte\":\n",
        "              logits = self.classifier_rte(z)\n",
        "            else:\n",
        "              logits = self.classifier_mrpc(z)\n",
        "            mean_logits = logits.mean(dim=0)\n",
        "            logits = logits.permute(1, 2, 0)\n",
        "        else:\n",
        "            if dataset_name == 0:\n",
        "              mean_logits = self.classifier_rte(mu)\n",
        "            else:\n",
        "              mean_logits = self.classifier_mrpc(mu)\n",
        "            #mean_logits = self.classifier(mu)\n",
        "            logits = mean_logits\n",
        "        return logits, mean_logits\n",
        "\n",
        "\n",
        "    def sampled_loss(self, logits, mean_logits, labels, sampling_type):\n",
        "        if sampling_type == \"iid\":\n",
        "            # During the training, computes the loss with the sampled embeddings.\n",
        "            if self.num_labels == 1:\n",
        "                #  We are doing regression\n",
        "                loss_fct = MSELoss()\n",
        "                loss = loss_fct(logits.view(-1, self.sample_size), labels[:, None].float().expand(-1, self.sample_size))\n",
        "                loss = torch.mean(loss, dim=-1)\n",
        "                loss = torch.mean(loss, dim=0)\n",
        "            else:\n",
        "                loss_fct = CrossEntropyLoss(reduce=False)\n",
        "                loss = loss_fct(logits, labels[:, None].expand(-1, self.sample_size))\n",
        "                loss = torch.mean(loss, dim=-1)\n",
        "                loss = torch.mean(loss, dim=0)\n",
        "        else:\n",
        "            # During test time, uses the average value for prediction.\n",
        "            if self.num_labels == 1:\n",
        "                loss_fct = MSELoss()\n",
        "                loss = loss_fct(mean_logits.view(-1), labels.float().view(-1))\n",
        "            else:\n",
        "                loss_fct = CrossEntropyLoss()\n",
        "                loss = loss_fct(mean_logits, labels)\n",
        "        return loss\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        sampling_type=\"iid\",\n",
        "        epoch=1,\n",
        "        **kwargs\n",
        "        #dataset_name=\"rte\",\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`, defaults to :obj:`None`):\n",
        "            Labels for computing the sequence classification/regression loss.\n",
        "            Indices should be in :obj:`[0, ..., config.num_labels - 1]`.\n",
        "            If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n",
        "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
        "    Returns:\n",
        "        :obj:`tuple(torch.FloatTensor)` comprising various elements depending on the configuration (:class:`~transformers.BertConfig`) and inputs:\n",
        "        loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when :obj:`label` is provided):\n",
        "            Classification (or regression if config.num_labels==1) loss.\n",
        "        logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, config.num_labels)`):\n",
        "            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
        "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_hidden_states=True``):\n",
        "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
        "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
        "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
        "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_attentions=True``):\n",
        "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape\n",
        "            :obj:`(batch_size, num_heads, sequence_length, sequence_length)`.\n",
        "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
        "            heads.\n",
        "    Examples::\n",
        "        from transformers import BertTokenizer, BertForSequenceClassification\n",
        "        import torch\n",
        "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
        "        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
        "        outputs = model(input_ids, labels=labels)\n",
        "        loss, logits = outputs[:2]\n",
        "        \"\"\"\n",
        "        #dataset_name=\"rte\"\n",
        "        #print(position_ids.item())\n",
        "        position_id = None\n",
        "        final_outputs = {}\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_id,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "        )\n",
        "        pooled_output = outputs[1]\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        loss = {}\n",
        "\n",
        "        if self.deterministic:\n",
        "            pooled_output = self.mlp(pooled_output)\n",
        "            mu, std = self.estimate(pooled_output, self.emb2mu, self.emb2std)\n",
        "            final_outputs[\"z\"] = mu\n",
        "            sampled_logits, logits = self.get_logits(mu, mu, sampling_type='argmax',dataset_name=kwargs[\"dataset_name\"]) # always deterministic\n",
        "            if labels is not None:\n",
        "                loss[\"loss\"] = self.sampled_loss(sampled_logits, logits, labels.view(-1), sampling_type='argmax')\n",
        "\n",
        "        elif self.ib:\n",
        "            if kwargs[\"dataset_name\"]==\"rte\":\n",
        "              pooled_output = self.mlp_rte(pooled_output)\n",
        "              batch_size = pooled_output.shape[0]\n",
        "              mu, std = self.estimate(pooled_output, self.emb2mu_rte, self.emb2std_rte)\n",
        "              mu_p = self.mu_p_rte.view(1, -1).expand(batch_size, -1)\n",
        "              std_p = torch.nn.functional.softplus(self.std_p_rte.view(1, -1).expand(batch_size, -1))\n",
        "            else:\n",
        "              pooled_output = self.mlp_mrpc(pooled_output)\n",
        "              batch_size = pooled_output.shape[0]\n",
        "              mu, std = self.estimate(pooled_output, self.emb2mu_mrpc, self.emb2std_mrpc)\n",
        "              mu_p = self.mu_p_mrpc.view(1, -1).expand(batch_size, -1)\n",
        "              std_p = torch.nn.functional.softplus(self.std_p_mrpc.view(1, -1).expand(batch_size, -1))\n",
        "            kl_loss = self.kl_div(mu, std, mu_p, std_p)\n",
        "            z = self.reparameterize(mu, std)\n",
        "            final_outputs[\"z\"] = mu\n",
        "\n",
        "            if self.kl_annealing == \"linear\":\n",
        "                beta = min(1.0, epoch*self.beta)\n",
        "                 \n",
        "            sampled_logits, logits = self.get_logits(z, mu, sampling_type,dataset_name=kwargs[\"dataset_name\"])\n",
        "            #print(labels)\n",
        "            if labels is not None:\n",
        "                if kwargs[\"label\"] is not None:\n",
        "                  ce_loss_rte = self.sampled_loss(kwargs[\"sampled_logits\"], kwargs[\"logits\"], kwargs[\"label\"].view(-1), sampling_type)\n",
        "                  total_loss_rte = ce_loss_rte + (beta if self.kl_annealing == \"linear\" else self.beta) * kwargs[\"kl_loss\"]\n",
        "                  ce_loss_mrpc = self.sampled_loss(sampled_logits, logits, labels.view(-1), sampling_type)\n",
        "                  total_loss_mrpc = ce_loss_mrpc + (beta if self.kl_annealing == \"linear\" else self.beta) * kl_loss\n",
        "                  total_loss = total_loss_rte + total_loss_mrpc\n",
        "                else:\n",
        "                  ce_loss = self.sampled_loss(sampled_logits, logits, labels.view(-1), sampling_type)\n",
        "                  total_loss = ce_loss + (beta if self.kl_annealing == \"linear\" else self.beta) * kl_loss\n",
        "                loss[\"loss\"] = total_loss\n",
        "        else:\n",
        "            final_outputs[\"z\"] = pooled_output\n",
        "            logits = self.classifier(pooled_output)\n",
        "            if labels is not None:\n",
        "                if self.num_labels == 1:\n",
        "                    #  We are doing regression\n",
        "                    loss_fct = MSELoss()\n",
        "                    loss[\"loss\"] = loss_fct(logits.view(-1), labels.float().view(-1))\n",
        "                else:\n",
        "                    loss_fct = CrossEntropyLoss()\n",
        "                    loss[\"loss\"] = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "                    \n",
        "        final_outputs.update({\"logits\": logits, \"loss\": loss, \"hidden_attention\": outputs[2:],\"sampled_logits\":sampled_logits,\"kl_loss\":kl_loss})\n",
        "        return final_outputs"
      ],
      "metadata": {
        "id": "dJ-7Jgo9-DOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RTE_data(torch.utils.data.Dataset):\n",
        "  def __init__(self,data_type):\n",
        "    self.labels = []\n",
        "    self.input_1 = []\n",
        "    self.input_2 = []\n",
        "    self.rte_train_iter = RTE(split=data_type)\n",
        "    for label,inp1,inp2 in self.rte_train_iter:\n",
        "      self.labels.append(label)\n",
        "      self.input_1.append(inp1)\n",
        "      self.input_2.append(inp2)\n",
        "    \n",
        "  def __getitem__(self,idx):\n",
        "    return self.labels[idx],self.input_1[idx],self.input_2[idx],\"rte\"\n",
        "  def __len__(self):\n",
        "    return len(self.labels)"
      ],
      "metadata": {
        "id": "MAezbgVI-9lR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MRPC_data(torch.utils.data.Dataset):\n",
        "  def __init__(self,data_type):\n",
        "    self.labels = []\n",
        "    self.input_1 = []\n",
        "    self.input_2 = []\n",
        "    \n",
        "    if data_type == \"train\" or data_type == \"dev\":\n",
        "      self.mrpc_train_iter = MRPC(split=\"train\")\n",
        "    else:\n",
        "      self.mrpc_train_iter = MRPC(split=data_type)\n",
        "    for label,inp1,inp2 in self.mrpc_train_iter:\n",
        "      self.labels.append(label)\n",
        "      self.input_1.append(inp1)\n",
        "      self.input_2.append(inp2)\n",
        "\n",
        "    print(int(len(self.labels)*0.8))\n",
        "    if data_type == \"train\":\n",
        "      self.labels = self.labels[:int(len(self.labels)*0.8)]\n",
        "      self.input_1 = self.input_1[:int(len(self.input_1)*0.8)]\n",
        "      self.input_2 = self.input_2[:int(len(self.input_2)*0.8)]\n",
        "    if data_type == \"dev\":\n",
        "      self.labels = self.labels[int(len(self.labels)*0.8):]\n",
        "      self.input_1 = self.input_1[int(len(self.input_1)*0.8):]\n",
        "      self.input_2 = self.input_2[int(len(self.input_2)*0.8):]\n",
        "  def __getitem__(self,idx):\n",
        "    return self.labels[idx],self.input_1[idx],self.input_2[idx],\"mrpc\"\n",
        "  def __len__(self):\n",
        "    return len(self.labels)"
      ],
      "metadata": {
        "id": "j6cXMmGr_MMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "from torch.utils.data.sampler import RandomSampler\n",
        "\n",
        "\n",
        "class BatchSchedulerSampler(torch.utils.data.sampler.Sampler):\n",
        "    \"\"\"\n",
        "    iterate over tasks and provide a random batch per task in each mini-batch\n",
        "    \"\"\"\n",
        "    def __init__(self, dataset, batch_size):\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.number_of_datasets = len(dataset.datasets)\n",
        "        self.largest_dataset_size = max([len(cur_dataset.labels) for cur_dataset in dataset.datasets])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.batch_size * math.ceil(self.largest_dataset_size / self.batch_size) * len(self.dataset.datasets)\n",
        "\n",
        "    def __iter__(self):\n",
        "        samplers_list = []\n",
        "        sampler_iterators = []\n",
        "        for dataset_idx in range(self.number_of_datasets):\n",
        "            cur_dataset = self.dataset.datasets[dataset_idx]\n",
        "            sampler = RandomSampler(cur_dataset)\n",
        "            samplers_list.append(sampler)\n",
        "            cur_sampler_iterator = sampler.__iter__()\n",
        "            sampler_iterators.append(cur_sampler_iterator)\n",
        "\n",
        "        push_index_val = [0] + self.dataset.cumulative_sizes[:-1]\n",
        "        step = self.batch_size * self.number_of_datasets\n",
        "        samples_to_grab = self.batch_size\n",
        "        # for this case we want to get all samples in dataset, this force us to resample from the smaller datasets\n",
        "        epoch_samples = self.largest_dataset_size * self.number_of_datasets\n",
        "\n",
        "        final_samples_list = []  # this is a list of indexes from the combined dataset\n",
        "        for _ in range(0, epoch_samples, step):\n",
        "            for i in range(self.number_of_datasets):\n",
        "                cur_batch_sampler = sampler_iterators[i]\n",
        "                cur_samples = []\n",
        "                for _ in range(samples_to_grab):\n",
        "                    try:\n",
        "                        cur_sample_org = cur_batch_sampler.__next__()\n",
        "                        cur_sample = cur_sample_org + push_index_val[i]\n",
        "                        cur_samples.append(cur_sample)\n",
        "                    except StopIteration:\n",
        "                        # got to the end of iterator - restart the iterator and continue to get samples\n",
        "                        # until reaching \"epoch_samples\"\n",
        "                        sampler_iterators[i] = samplers_list[i].__iter__()\n",
        "                        cur_batch_sampler = sampler_iterators[i]\n",
        "                        cur_sample_org = cur_batch_sampler.__next__()\n",
        "                        cur_sample = cur_sample_org + push_index_val[i]\n",
        "                        cur_samples.append(cur_sample)\n",
        "                final_samples_list.extend(cur_samples)\n",
        "\n",
        "        return iter(final_samples_list)"
      ],
      "metadata": {
        "id": "IAH91c4J_MnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rte_data = RTE_data(\"train\")\n",
        "mrpc_data = MRPC_data(\"train\")\n",
        "concat = ConcatDataset([rte_data,mrpc_data])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiqNaFfO_Pe0",
        "outputId": "4cfd09a4-9b40-45b1-8fd2-357e6090396c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3260\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(dataset=concat,\n",
        "                                         sampler=BatchSchedulerSampler(dataset=concat,\n",
        "                                                                       batch_size=32),\n",
        "                                         batch_size=32,\n",
        "                                         shuffle=False)"
      ],
      "metadata": {
        "id": "MCo9Utvv_Rs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "phhtiZH7Pgp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_rte_data = RTE_data(\"dev\")\n",
        "dev_mrpc_data = MRPC_data(\"dev\")\n",
        "dev_concat = ConcatDataset([dev_rte_data,dev_mrpc_data])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZjZFq1M_TML",
        "outputId": "8427e117-87bc-4d3b-eac8-927198ca48a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3260\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dev_dataloader = torch.utils.data.DataLoader(dataset=dev_concat,\n",
        "                                         sampler=BatchSchedulerSampler(dataset=dev_concat,\n",
        "                                                                       batch_size=32),\n",
        "                                         batch_size=32,\n",
        "                                         shuffle=False)"
      ],
      "metadata": {
        "id": "E9OwUwsz_VUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_mrpc_data = MRPC_data(\"test\")\n",
        "mrpc_test_dataloader = torch.utils.data.DataLoader(dataset=test_mrpc_data,\n",
        "                                         batch_size=32,\n",
        "                                         shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSvMSFZC_XDV",
        "outputId": "335d3e29-85f4-4693-9864-fe4b7bf32e9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1380\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "config = BertConfig.from_pretrained(\n",
        "        \"bert-base-uncased\",\n",
        "        num_labels=2)\n",
        "vibert = BertForSequenceClassification.from_pretrained(\n",
        "        \"bert-base-uncased\",\n",
        "        config=config\n",
        ")"
      ],
      "metadata": {
        "id": "_m3DcWL9_btK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9aeec49f-bd9d-49fd-b873-96abdcaf5525"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier_mrpc.bias', 'emb2std_mrpc.bias', 'emb2mu_mrpc.weight', 'std_p_mrpc', 'std_p_rte', 'emb2mu_rte.weight', 'emb2std_mrpc.weight', 'classifier_mrpc.weight', 'classifier_rte.weight', 'mlp_mrpc.0.bias', 'emb2std_rte.weight', 'classifier_rte.bias', 'emb2mu_mrpc.bias', 'mu_p_rte', 'mlp_rte.0.bias', 'mlp_rte.0.weight', 'mlp_mrpc.2.weight', 'mu_p_mrpc', 'emb2mu_rte.bias', 'mlp_mrpc.0.weight', 'mlp_mrpc.2.bias', 'mlp_rte.2.weight', 'mlp_rte.2.bias', 'emb2std_rte.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in vibert.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWuMP4IJ_s4c",
        "outputId": "334de5d8-bc75-424e-8d2c-3c7a45314831"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mu_p_rte\n",
            "std_p_rte\n",
            "mu_p_mrpc\n",
            "std_p_mrpc\n",
            "bert.embeddings.word_embeddings.weight\n",
            "bert.embeddings.position_embeddings.weight\n",
            "bert.embeddings.token_type_embeddings.weight\n",
            "bert.embeddings.LayerNorm.weight\n",
            "bert.embeddings.LayerNorm.bias\n",
            "bert.encoder.layer.0.attention.self.query.weight\n",
            "bert.encoder.layer.0.attention.self.query.bias\n",
            "bert.encoder.layer.0.attention.self.key.weight\n",
            "bert.encoder.layer.0.attention.self.key.bias\n",
            "bert.encoder.layer.0.attention.self.value.weight\n",
            "bert.encoder.layer.0.attention.self.value.bias\n",
            "bert.encoder.layer.0.attention.output.dense.weight\n",
            "bert.encoder.layer.0.attention.output.dense.bias\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
            "bert.encoder.layer.0.intermediate.dense.weight\n",
            "bert.encoder.layer.0.intermediate.dense.bias\n",
            "bert.encoder.layer.0.output.dense.weight\n",
            "bert.encoder.layer.0.output.dense.bias\n",
            "bert.encoder.layer.0.output.LayerNorm.weight\n",
            "bert.encoder.layer.0.output.LayerNorm.bias\n",
            "bert.encoder.layer.1.attention.self.query.weight\n",
            "bert.encoder.layer.1.attention.self.query.bias\n",
            "bert.encoder.layer.1.attention.self.key.weight\n",
            "bert.encoder.layer.1.attention.self.key.bias\n",
            "bert.encoder.layer.1.attention.self.value.weight\n",
            "bert.encoder.layer.1.attention.self.value.bias\n",
            "bert.encoder.layer.1.attention.output.dense.weight\n",
            "bert.encoder.layer.1.attention.output.dense.bias\n",
            "bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
            "bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
            "bert.encoder.layer.1.intermediate.dense.weight\n",
            "bert.encoder.layer.1.intermediate.dense.bias\n",
            "bert.encoder.layer.1.output.dense.weight\n",
            "bert.encoder.layer.1.output.dense.bias\n",
            "bert.encoder.layer.1.output.LayerNorm.weight\n",
            "bert.encoder.layer.1.output.LayerNorm.bias\n",
            "bert.encoder.layer.2.attention.self.query.weight\n",
            "bert.encoder.layer.2.attention.self.query.bias\n",
            "bert.encoder.layer.2.attention.self.key.weight\n",
            "bert.encoder.layer.2.attention.self.key.bias\n",
            "bert.encoder.layer.2.attention.self.value.weight\n",
            "bert.encoder.layer.2.attention.self.value.bias\n",
            "bert.encoder.layer.2.attention.output.dense.weight\n",
            "bert.encoder.layer.2.attention.output.dense.bias\n",
            "bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
            "bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
            "bert.encoder.layer.2.intermediate.dense.weight\n",
            "bert.encoder.layer.2.intermediate.dense.bias\n",
            "bert.encoder.layer.2.output.dense.weight\n",
            "bert.encoder.layer.2.output.dense.bias\n",
            "bert.encoder.layer.2.output.LayerNorm.weight\n",
            "bert.encoder.layer.2.output.LayerNorm.bias\n",
            "bert.encoder.layer.3.attention.self.query.weight\n",
            "bert.encoder.layer.3.attention.self.query.bias\n",
            "bert.encoder.layer.3.attention.self.key.weight\n",
            "bert.encoder.layer.3.attention.self.key.bias\n",
            "bert.encoder.layer.3.attention.self.value.weight\n",
            "bert.encoder.layer.3.attention.self.value.bias\n",
            "bert.encoder.layer.3.attention.output.dense.weight\n",
            "bert.encoder.layer.3.attention.output.dense.bias\n",
            "bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
            "bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
            "bert.encoder.layer.3.intermediate.dense.weight\n",
            "bert.encoder.layer.3.intermediate.dense.bias\n",
            "bert.encoder.layer.3.output.dense.weight\n",
            "bert.encoder.layer.3.output.dense.bias\n",
            "bert.encoder.layer.3.output.LayerNorm.weight\n",
            "bert.encoder.layer.3.output.LayerNorm.bias\n",
            "bert.encoder.layer.4.attention.self.query.weight\n",
            "bert.encoder.layer.4.attention.self.query.bias\n",
            "bert.encoder.layer.4.attention.self.key.weight\n",
            "bert.encoder.layer.4.attention.self.key.bias\n",
            "bert.encoder.layer.4.attention.self.value.weight\n",
            "bert.encoder.layer.4.attention.self.value.bias\n",
            "bert.encoder.layer.4.attention.output.dense.weight\n",
            "bert.encoder.layer.4.attention.output.dense.bias\n",
            "bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
            "bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
            "bert.encoder.layer.4.intermediate.dense.weight\n",
            "bert.encoder.layer.4.intermediate.dense.bias\n",
            "bert.encoder.layer.4.output.dense.weight\n",
            "bert.encoder.layer.4.output.dense.bias\n",
            "bert.encoder.layer.4.output.LayerNorm.weight\n",
            "bert.encoder.layer.4.output.LayerNorm.bias\n",
            "bert.encoder.layer.5.attention.self.query.weight\n",
            "bert.encoder.layer.5.attention.self.query.bias\n",
            "bert.encoder.layer.5.attention.self.key.weight\n",
            "bert.encoder.layer.5.attention.self.key.bias\n",
            "bert.encoder.layer.5.attention.self.value.weight\n",
            "bert.encoder.layer.5.attention.self.value.bias\n",
            "bert.encoder.layer.5.attention.output.dense.weight\n",
            "bert.encoder.layer.5.attention.output.dense.bias\n",
            "bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
            "bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
            "bert.encoder.layer.5.intermediate.dense.weight\n",
            "bert.encoder.layer.5.intermediate.dense.bias\n",
            "bert.encoder.layer.5.output.dense.weight\n",
            "bert.encoder.layer.5.output.dense.bias\n",
            "bert.encoder.layer.5.output.LayerNorm.weight\n",
            "bert.encoder.layer.5.output.LayerNorm.bias\n",
            "bert.encoder.layer.6.attention.self.query.weight\n",
            "bert.encoder.layer.6.attention.self.query.bias\n",
            "bert.encoder.layer.6.attention.self.key.weight\n",
            "bert.encoder.layer.6.attention.self.key.bias\n",
            "bert.encoder.layer.6.attention.self.value.weight\n",
            "bert.encoder.layer.6.attention.self.value.bias\n",
            "bert.encoder.layer.6.attention.output.dense.weight\n",
            "bert.encoder.layer.6.attention.output.dense.bias\n",
            "bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
            "bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
            "bert.encoder.layer.6.intermediate.dense.weight\n",
            "bert.encoder.layer.6.intermediate.dense.bias\n",
            "bert.encoder.layer.6.output.dense.weight\n",
            "bert.encoder.layer.6.output.dense.bias\n",
            "bert.encoder.layer.6.output.LayerNorm.weight\n",
            "bert.encoder.layer.6.output.LayerNorm.bias\n",
            "bert.encoder.layer.7.attention.self.query.weight\n",
            "bert.encoder.layer.7.attention.self.query.bias\n",
            "bert.encoder.layer.7.attention.self.key.weight\n",
            "bert.encoder.layer.7.attention.self.key.bias\n",
            "bert.encoder.layer.7.attention.self.value.weight\n",
            "bert.encoder.layer.7.attention.self.value.bias\n",
            "bert.encoder.layer.7.attention.output.dense.weight\n",
            "bert.encoder.layer.7.attention.output.dense.bias\n",
            "bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
            "bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
            "bert.encoder.layer.7.intermediate.dense.weight\n",
            "bert.encoder.layer.7.intermediate.dense.bias\n",
            "bert.encoder.layer.7.output.dense.weight\n",
            "bert.encoder.layer.7.output.dense.bias\n",
            "bert.encoder.layer.7.output.LayerNorm.weight\n",
            "bert.encoder.layer.7.output.LayerNorm.bias\n",
            "bert.encoder.layer.8.attention.self.query.weight\n",
            "bert.encoder.layer.8.attention.self.query.bias\n",
            "bert.encoder.layer.8.attention.self.key.weight\n",
            "bert.encoder.layer.8.attention.self.key.bias\n",
            "bert.encoder.layer.8.attention.self.value.weight\n",
            "bert.encoder.layer.8.attention.self.value.bias\n",
            "bert.encoder.layer.8.attention.output.dense.weight\n",
            "bert.encoder.layer.8.attention.output.dense.bias\n",
            "bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
            "bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
            "bert.encoder.layer.8.intermediate.dense.weight\n",
            "bert.encoder.layer.8.intermediate.dense.bias\n",
            "bert.encoder.layer.8.output.dense.weight\n",
            "bert.encoder.layer.8.output.dense.bias\n",
            "bert.encoder.layer.8.output.LayerNorm.weight\n",
            "bert.encoder.layer.8.output.LayerNorm.bias\n",
            "bert.encoder.layer.9.attention.self.query.weight\n",
            "bert.encoder.layer.9.attention.self.query.bias\n",
            "bert.encoder.layer.9.attention.self.key.weight\n",
            "bert.encoder.layer.9.attention.self.key.bias\n",
            "bert.encoder.layer.9.attention.self.value.weight\n",
            "bert.encoder.layer.9.attention.self.value.bias\n",
            "bert.encoder.layer.9.attention.output.dense.weight\n",
            "bert.encoder.layer.9.attention.output.dense.bias\n",
            "bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
            "bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
            "bert.encoder.layer.9.intermediate.dense.weight\n",
            "bert.encoder.layer.9.intermediate.dense.bias\n",
            "bert.encoder.layer.9.output.dense.weight\n",
            "bert.encoder.layer.9.output.dense.bias\n",
            "bert.encoder.layer.9.output.LayerNorm.weight\n",
            "bert.encoder.layer.9.output.LayerNorm.bias\n",
            "bert.encoder.layer.10.attention.self.query.weight\n",
            "bert.encoder.layer.10.attention.self.query.bias\n",
            "bert.encoder.layer.10.attention.self.key.weight\n",
            "bert.encoder.layer.10.attention.self.key.bias\n",
            "bert.encoder.layer.10.attention.self.value.weight\n",
            "bert.encoder.layer.10.attention.self.value.bias\n",
            "bert.encoder.layer.10.attention.output.dense.weight\n",
            "bert.encoder.layer.10.attention.output.dense.bias\n",
            "bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
            "bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
            "bert.encoder.layer.10.intermediate.dense.weight\n",
            "bert.encoder.layer.10.intermediate.dense.bias\n",
            "bert.encoder.layer.10.output.dense.weight\n",
            "bert.encoder.layer.10.output.dense.bias\n",
            "bert.encoder.layer.10.output.LayerNorm.weight\n",
            "bert.encoder.layer.10.output.LayerNorm.bias\n",
            "bert.encoder.layer.11.attention.self.query.weight\n",
            "bert.encoder.layer.11.attention.self.query.bias\n",
            "bert.encoder.layer.11.attention.self.key.weight\n",
            "bert.encoder.layer.11.attention.self.key.bias\n",
            "bert.encoder.layer.11.attention.self.value.weight\n",
            "bert.encoder.layer.11.attention.self.value.bias\n",
            "bert.encoder.layer.11.attention.output.dense.weight\n",
            "bert.encoder.layer.11.attention.output.dense.bias\n",
            "bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
            "bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
            "bert.encoder.layer.11.intermediate.dense.weight\n",
            "bert.encoder.layer.11.intermediate.dense.bias\n",
            "bert.encoder.layer.11.output.dense.weight\n",
            "bert.encoder.layer.11.output.dense.bias\n",
            "bert.encoder.layer.11.output.LayerNorm.weight\n",
            "bert.encoder.layer.11.output.LayerNorm.bias\n",
            "bert.pooler.dense.weight\n",
            "bert.pooler.dense.bias\n",
            "mlp_rte.0.weight\n",
            "mlp_rte.0.bias\n",
            "mlp_rte.2.weight\n",
            "mlp_rte.2.bias\n",
            "mlp_mrpc.0.weight\n",
            "mlp_mrpc.0.bias\n",
            "mlp_mrpc.2.weight\n",
            "mlp_mrpc.2.bias\n",
            "emb2mu_rte.weight\n",
            "emb2mu_rte.bias\n",
            "emb2std_rte.weight\n",
            "emb2std_rte.bias\n",
            "emb2mu_mrpc.weight\n",
            "emb2mu_mrpc.bias\n",
            "emb2std_mrpc.weight\n",
            "emb2std_mrpc.bias\n",
            "classifier_rte.weight\n",
            "classifier_rte.bias\n",
            "classifier_mrpc.weight\n",
            "classifier_mrpc.bias\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "optimizer_grouped_parameters = [\n",
        "        {\n",
        "            \"params\": [p for n, p in vibert.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "            \"weight_decay\": 0.0,\n",
        "        },\n",
        "        {\"params\": [p for n, p in vibert.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
        "]\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=5e-5, eps=1e-8) #lr was 5e-5\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0,\n",
        "                                                num_training_steps=len(train_dataloader)*50)"
      ],
      "metadata": {
        "id": "ja_B-UFz_64d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18aeeb6b-e9ff-493e-fd88-9b593bea2e8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning) "
      ],
      "metadata": {
        "id": "Th8-M4V4AEhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "transformers.logging.set_verbosity_error()"
      ],
      "metadata": {
        "id": "R0OVE3QzAI3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    #if args.n_gpu > 0:\n",
        "    #    torch.cuda.manual_seed_all(args.seed)\n",
        "    torch.cuda.manual_seed(seed)"
      ],
      "metadata": {
        "id": "5eEWZxSiAKUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score,f1_score\n",
        "\n",
        "n_epochs = 50 #10\n",
        "device=\"cuda\"\n",
        "#vibert.train()\n",
        "vibert.to(device)\n",
        "vibert.zero_grad()\n",
        "set_seed(2022)\n",
        "train_loss_rte=[]\n",
        "train_loss_mrpc=[]\n",
        "dev_loss_rte=[]\n",
        "dev_loss_mrpc=[]\n",
        "train_acc_rte=[]\n",
        "train_acc_mrpc=[]\n",
        "dev_acc_rte=[]\n",
        "dev_acc_mrpc=[]\n",
        "total_loss=[]\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  total_train_loss_rte = 0\n",
        "  total_train_loss_mrpc = 0\n",
        "  total_dev_loss_rte = 0\n",
        "  total_dev_loss_mrpc = 0\n",
        "  total_train_acc_rte = 0\n",
        "  total_train_acc_mrpc = 0\n",
        "  total_dev_acc_rte = 0\n",
        "  total_dev_acc_mrpc = 0\n",
        "  total_train_loss=0\n",
        "  vibert.train()\n",
        "  c1_train=0\n",
        "  c2_train=0\n",
        "  c=0\n",
        "  logs=None\n",
        "  labs = None\n",
        "  sampled_labs = None\n",
        "  kl_loss= None\n",
        "  for labels,inp1,inp2,st in tqdm(train_dataloader):\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    batch = tokenizer(text=inp1,text_pair=inp2,max_length=128,truncation=True,padding=True,add_special_tokens=True,is_split_into_words=False,return_tensors='pt')\n",
        "    batch.to(device)\n",
        "\n",
        "    #with torch.set_grad_enabled(True):\n",
        "    out = vibert(batch[\"input_ids\"],token_type_ids=batch[\"token_type_ids\"], \n",
        "                            attention_mask=batch[\"attention_mask\"],labels=labels.to(device),dataset_name=st[0],logits=logs,label=labs,sampled_logits=sampled_labs,kl_loss=kl_loss)\n",
        "    \n",
        "    #loss = (loss*0.5)/2 \n",
        "    logs= out[\"logits\"].to(device)\n",
        "    labs=labels.to(device)\n",
        "    sampled_labs = out[\"sampled_logits\"].to(device)\n",
        "    kl_loss = out[\"kl_loss\"]\n",
        "    f=0\n",
        "    if st[0]==\"rte\":\n",
        "      c1_train+=1\n",
        "    if st[0]==\"mrpc\":\n",
        "      c2_train+=1\n",
        "    \n",
        "    if c1_train == c2_train:\n",
        "      loss = out[\"loss\"][\"loss\"]  \n",
        "      total_train_loss+=loss.item()\n",
        "      c+=1\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      scheduler.step()  # Update learning rate schedule\n",
        "      vibert.zero_grad()\n",
        "      logs=None\n",
        "      labs = None\n",
        "      sampled_labs=None\n",
        "      kl_loss = None\n",
        "      \n",
        "      f=1\n",
        "      torch.nn.utils.clip_grad_norm_(vibert.parameters(), 1.0)\n",
        "    if st[0]==\"rte\":\n",
        "        out = vibert(batch[\"input_ids\"],token_type_ids=batch[\"token_type_ids\"], \n",
        "                            attention_mask=batch[\"attention_mask\"],labels=labels.to(device),dataset_name=st[0],logits=None,label=None,sampled_logits=None,kl_loss=None)\n",
        "        loss = out[\"loss\"][\"loss\"]\n",
        "        total_train_loss_rte += loss.item()\n",
        "        total_train_acc_rte += accuracy_score(labels.cpu(),torch.argmax(out[\"logits\"],1).cpu())  \n",
        "    if st[0]==\"mrpc\":\n",
        "      #total_train_loss_mrpc += loss.item()\n",
        "      out = vibert(batch[\"input_ids\"],token_type_ids=batch[\"token_type_ids\"], \n",
        "                            attention_mask=batch[\"attention_mask\"],labels=labels.to(device),dataset_name=st[0],logits=None,label=None,sampled_logits=None,kl_loss=None)\n",
        "      loss = out[\"loss\"][\"loss\"]\n",
        "      total_train_loss_mrpc += loss.item()\n",
        "      total_train_acc_mrpc += accuracy_score(labels.cpu(),torch.argmax(out[\"logits\"],1).cpu())\n",
        "    #if f==0 and c1_train+c2_train>=len(train_dataloader):\n",
        "    #  optimizer.step()\n",
        "    #  scheduler.step()  # Update learning rate schedule\n",
        "    #  vibert.zero_grad()\n",
        "  print(\"Epoch \"+str(epoch)+\" RTE Train Loss: \"+str(total_train_loss/len(train_dataloader)))\n",
        "  print(\"Epoch \"+str(epoch)+\" RTE Train Loss: \"+str(total_train_loss_rte/c1_train)+\" MRPC Train Loss: \"+str(total_train_loss_mrpc/c2_train))\n",
        "  print(\" RTE Train Acc: \"+str(total_train_acc_rte/c1_train)+\" MRPC Train Acc: \"+str(total_train_acc_mrpc/c2_train))\n",
        "  total_loss.append(total_train_loss/c)\n",
        "  train_loss_rte.append(total_train_loss_rte/c1_train)\n",
        "  train_loss_mrpc.append(total_train_loss_mrpc/c2_train)\n",
        "  train_acc_rte.append(total_train_acc_rte/c1_train)\n",
        "  train_acc_mrpc.append(total_train_acc_mrpc/c2_train)\n",
        "  \n",
        "\n",
        "  \n",
        "  vibert.eval()\n",
        "  c1_dev=0\n",
        "  c2_dev=0\n",
        "  for labels,inp1,inp2,st in tqdm(dev_dataloader):\n",
        "    \n",
        "    batch = tokenizer(text=inp1,text_pair=inp2,max_length=128,truncation=True,padding=True,is_split_into_words=False,return_tensors='pt')\n",
        "    batch.to(device)\n",
        "    \n",
        "    out = vibert(batch[\"input_ids\"],token_type_ids=batch[\"token_type_ids\"], \n",
        "                             attention_mask=batch[\"attention_mask\"],labels=labels.to(device),dataset_name=st[0],logits=None,label=None,sampled_logits=None,kl_loss=None)\n",
        "    loss = out[\"loss\"][\"loss\"]\n",
        "    if st[0]==\"rte\":\n",
        "      total_dev_loss_rte += loss.item()\n",
        "      total_dev_acc_rte += accuracy_score(labels.cpu(),torch.argmax(out[\"logits\"],1).cpu())\n",
        "      c1_dev+=1\n",
        "    if st[0]==\"mrpc\":\n",
        "      total_dev_loss_mrpc += loss.item()\n",
        "      total_dev_acc_mrpc += accuracy_score(labels.cpu(),torch.argmax(out[\"logits\"],1).cpu())\n",
        "      c2_dev+=1\n",
        "     \n",
        "  print(\"Epoch \"+str(epoch)+\" RTE Val Loss: \"+str(total_dev_loss_rte/c1_dev)+\" MRPC Val Loss: \"+str(total_dev_loss_mrpc/c2_dev))\n",
        "  print(\" RTE Val Acc: \"+str(total_dev_acc_rte/c1_dev)+\" MRPC Val Acc: \"+str(total_dev_acc_mrpc/c2_dev))\n",
        "  \n",
        "  dev_loss_rte.append(total_dev_loss_rte/c1_dev)\n",
        "  dev_loss_mrpc.append(total_dev_loss_mrpc/c2_dev)\n",
        "  dev_acc_rte.append(total_dev_acc_rte/c1_dev)\n",
        "  dev_acc_mrpc.append(total_dev_acc_mrpc/c2_dev)\n",
        "  torch.save(vibert.state_dict(), \"vibert_70\")\n",
        "\n",
        "\n",
        "print(\"Total_train_loss : \",total_loss)\n",
        "print(\"RTE Train Loss: \",train_loss_rte)\n",
        "print(\"MRPC Train Loss: \",train_loss_mrpc)\n",
        "print(\"RTE Dev Loss: \",dev_loss_rte)\n",
        "print(\"MRPC Dev Loss: \",dev_loss_mrpc)\n",
        "print(\"RTE Train Acc: \",train_acc_rte)\n",
        "print(\"MRPC Train Acc: \",train_acc_mrpc)\n",
        "print(\"RTE Dev Acc: \",dev_acc_rte)\n",
        "print(\"MRPC Dev Acc: \",dev_acc_mrpc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijaizK5UALzt",
        "outputId": "664d5df1-8990-4a16-eb29-70190cd1abba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [02:41<00:00,  1.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 RTE Train Loss: 1.251426648275525\n",
            "Epoch 0 RTE Train Loss: 1.334515515495749 MRPC Train Loss: 1.149019192831189\n",
            " RTE Train Acc: 0.49356617647058826 MRPC Train Acc: 0.6617647058823529\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:14<00:00,  3.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 RTE Val Loss: 0.7320397129425635 MRPC Val Loss: 0.6699261138072381\n",
            " RTE Val Acc: 0.49399038461538464 MRPC Val Acc: 0.6899038461538461\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [02:37<00:00,  1.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 RTE Train Loss: 0.6974033999676797\n",
            "Epoch 1 RTE Train Loss: 0.7292657798411799 MRPC Train Loss: 0.66376651560559\n",
            " RTE Train Acc: 0.4806985294117647 MRPC Train Acc: 0.6715686274509803\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:12<00:00,  4.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 RTE Val Loss: 0.7169663401750418 MRPC Val Loss: 0.6410394987234702\n",
            " RTE Val Acc: 0.4963942307692308 MRPC Val Acc: 0.6911057692307693\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [02:36<00:00,  1.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 RTE Train Loss: 0.6871925943038043\n",
            "Epoch 2 RTE Train Loss: 0.7284209564620373 MRPC Train Loss: 0.6338868497633466\n",
            " RTE Train Acc: 0.49019607843137253 MRPC Train Acc: 0.6801470588235294\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:11<00:00,  4.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 RTE Val Loss: 0.7105068060067984 MRPC Val Loss: 0.5944715887308121\n",
            " RTE Val Acc: 0.5420673076923077 MRPC Val Acc: 0.6947115384615384\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [02:36<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 RTE Train Loss: 0.6129595613947102\n",
            "Epoch 3 RTE Train Loss: 0.7134610239197227 MRPC Train Loss: 0.46655357935849356\n",
            " RTE Train Acc: 0.5208333333333334 MRPC Train Acc: 0.8207720588235294\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:11<00:00,  4.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 RTE Val Loss: 0.7115780550699967 MRPC Val Loss: 0.48722034578139967\n",
            " RTE Val Acc: 0.49158653846153844 MRPC Val Acc: 0.7932692307692307\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [02:36<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 RTE Train Loss: 0.49766770355841694\n",
            "Epoch 4 RTE Train Loss: 0.6715581770621094 MRPC Train Loss: 0.26490774845667914\n",
            " RTE Train Acc: 0.6259191176470589 MRPC Train Acc: 0.9283088235294118\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:11<00:00,  4.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 RTE Val Loss: 0.6987266701001388 MRPC Val Loss: 0.4553659546833772\n",
            " RTE Val Acc: 0.6213942307692307 MRPC Val Acc: 0.8149038461538461\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [02:36<00:00,  1.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 RTE Train Loss: 0.3730382128965621\n",
            "Epoch 5 RTE Train Loss: 0.5394690296813553 MRPC Train Loss: 0.17951090772654496\n",
            " RTE Train Acc: 0.7775735294117647 MRPC Train Acc: 0.9635416666666666\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:11<00:00,  4.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 RTE Val Loss: 0.918610882300597 MRPC Val Loss: 0.5434417569866548\n",
            " RTE Val Acc: 0.5865384615384616 MRPC Val Acc: 0.8173076923076923\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [02:36<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 RTE Train Loss: 0.24040538934515973\n",
            "Epoch 6 RTE Train Loss: 0.3419480626197422 MRPC Train Loss: 0.12408912609166958\n",
            " RTE Train Acc: 0.8863357843137255 MRPC Train Acc: 0.9816176470588235\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:11<00:00,  4.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 RTE Val Loss: 1.1552517459942744 MRPC Val Loss: 0.5719578495392432\n",
            " RTE Val Acc: 0.5480769230769231 MRPC Val Acc: 0.8245192307692307\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [02:36<00:00,  1.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 RTE Train Loss: 0.1847436151229868\n",
            "Epoch 7 RTE Train Loss: 0.23742316823964024 MRPC Train Loss: 0.10897611015859772\n",
            " RTE Train Acc: 0.9390318627450981 MRPC Train Acc: 0.9840686274509803\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:11<00:00,  4.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 RTE Val Loss: 1.1517417362103095 MRPC Val Loss: 0.608026860998227\n",
            " RTE Val Acc: 0.6021634615384616 MRPC Val Acc: 0.8161057692307693\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [02:36<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 RTE Train Loss: 0.14686197458821185\n",
            "Epoch 8 RTE Train Loss: 0.18098012233773866 MRPC Train Loss: 0.09944964302521125\n",
            " RTE Train Acc: 0.9620098039215687 MRPC Train Acc: 0.9846813725490197\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:11<00:00,  4.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 RTE Val Loss: 1.2255823795612042 MRPC Val Loss: 0.6317977561400487\n",
            " RTE Val Acc: 0.5985576923076923 MRPC Val Acc: 0.8125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [02:36<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 RTE Train Loss: 0.11647131567930474\n",
            "Epoch 9 RTE Train Loss: 0.14024771932585567 MRPC Train Loss: 0.08129944820322242\n",
            " RTE Train Acc: 0.9742647058823529 MRPC Train Acc: 0.991421568627451\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:11<00:00,  4.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 RTE Val Loss: 1.3267897711350367 MRPC Val Loss: 0.6382897215393873\n",
            " RTE Val Acc: 0.5973557692307693 MRPC Val Acc: 0.8137019230769231\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [02:36<00:00,  1.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 RTE Train Loss: 0.10886948993977379\n",
            "Epoch 10 RTE Train Loss: 0.13493443656639725 MRPC Train Loss: 0.07483405375159254\n",
            " RTE Train Acc: 0.9751838235294118 MRPC Train Acc: 0.9920343137254902\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:11<00:00,  4.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 RTE Val Loss: 1.2916511595249176 MRPC Val Loss: 0.6520576041478378\n",
            " RTE Val Acc: 0.5985576923076923 MRPC Val Acc: 0.8245192307692307\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [02:35<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 RTE Train Loss: 0.09586257392577097\n",
            "Epoch 11 RTE Train Loss: 0.11734684322978936 MRPC Train Loss: 0.0657260703543822\n",
            " RTE Train Acc: 0.9813112745098039 MRPC Train Acc: 0.9932598039215687\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:11<00:00,  4.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 RTE Val Loss: 1.3559237512258382 MRPC Val Loss: 0.7373727170320657\n",
            " RTE Val Acc: 0.6009615384615384 MRPC Val Acc: 0.8052884615384616\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [02:35<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 RTE Train Loss: 0.08543526362992969\n",
            "Epoch 12 RTE Train Loss: 0.10482775595258265 MRPC Train Loss: 0.062140238270455714\n",
            " RTE Train Acc: 0.9837622549019608 MRPC Train Acc: 0.9944852941176471\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:11<00:00,  4.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 RTE Val Loss: 1.4887432914513807 MRPC Val Loss: 0.7514303567317816\n",
            " RTE Val Acc: 0.6021634615384616 MRPC Val Acc: 0.8149038461538461\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [02:36<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13 RTE Train Loss: 0.07917615427982573\n",
            "Epoch 13 RTE Train Loss: 0.09405309553532039 MRPC Train Loss: 0.0629663849797319\n",
            " RTE Train Acc: 0.984375 MRPC Train Acc: 0.9932598039215687\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:11<00:00,  4.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13 RTE Val Loss: 1.3826494148144355 MRPC Val Loss: 0.740893294604925\n",
            " RTE Val Acc: 0.6153846153846154 MRPC Val Acc: 0.8040865384615384\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [02:36<00:00,  1.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14 RTE Train Loss: 0.06506900763248696\n",
            "Epoch 14 RTE Train Loss: 0.07923773303627968 MRPC Train Loss: 0.05648158203956543\n",
            " RTE Train Acc: 0.9874387254901961 MRPC Train Acc: 0.9947916666666666\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:11<00:00,  4.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14 RTE Val Loss: 1.7487884163856506 MRPC Val Loss: 0.7638280277068799\n",
            " RTE Val Acc: 0.5877403846153846 MRPC Val Acc: 0.8052884615384616\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [02:36<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15 RTE Train Loss: 0.06109720651133388\n",
            "Epoch 15 RTE Train Loss: 0.06884033557977162 MRPC Train Loss: 0.05395722995493926\n",
            " RTE Train Acc: 0.9883578431372549 MRPC Train Acc: 0.9944852941176471\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:11<00:00,  4.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15 RTE Val Loss: 1.7620424261459937 MRPC Val Loss: 0.7328039178481469\n",
            " RTE Val Acc: 0.5889423076923077 MRPC Val Acc: 0.8100961538461539\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [02:35<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16 RTE Train Loss: 0.07429301647432879\n",
            "Epoch 16 RTE Train Loss: 0.08767319247857028 MRPC Train Loss: 0.05786482199076928\n",
            " RTE Train Acc: 0.9856004901960784 MRPC Train Acc: 0.992953431372549\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:11<00:00,  4.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16 RTE Val Loss: 1.547631025314331 MRPC Val Loss: 0.6707254017774875\n",
            " RTE Val Acc: 0.5913461538461539 MRPC Val Acc: 0.8161057692307693\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [02:36<00:00,  1.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17 RTE Train Loss: 0.05754311780865286\n",
            "Epoch 17 RTE Train Loss: 0.0644712114670113 MRPC Train Loss: 0.04655566040937807\n",
            " RTE Train Acc: 0.9926470588235294 MRPC Train Acc: 0.9947916666666666\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:11<00:00,  4.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17 RTE Val Loss: 1.6331771153670092 MRPC Val Loss: 0.6741555447761829\n",
            " RTE Val Acc: 0.5865384615384616 MRPC Val Acc: 0.8245192307692307\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [02:36<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18 RTE Train Loss: 0.0586700182700274\n",
            "Epoch 18 RTE Train Loss: 0.06295476539754401 MRPC Train Loss: 0.047750954917979004\n",
            " RTE Train Acc: 0.9917279411764706 MRPC Train Acc: 0.9947916666666666\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:11<00:00,  4.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18 RTE Val Loss: 1.660010617512923 MRPC Val Loss: 0.7625342435561694\n",
            " RTE Val Acc: 0.5757211538461539 MRPC Val Acc: 0.8245192307692307\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [02:36<00:00,  1.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19 RTE Train Loss: 0.04991016494950243\n",
            "Epoch 19 RTE Train Loss: 0.056782796248501424 MRPC Train Loss: 0.042858637303260026\n",
            " RTE Train Acc: 0.9932598039215687 MRPC Train Acc: 0.9957107843137255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:11<00:00,  4.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19 RTE Val Loss: 1.6736480731230516 MRPC Val Loss: 0.8446574004796835\n",
            " RTE Val Acc: 0.59375 MRPC Val Acc: 0.8016826923076923\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [02:35<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20 RTE Train Loss: 0.043789789226709626\n",
            "Epoch 20 RTE Train Loss: 0.04572204270345323 MRPC Train Loss: 0.039806607562829464\n",
            " RTE Train Acc: 0.9957107843137255 MRPC Train Acc: 0.9960171568627451\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:11<00:00,  4.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20 RTE Val Loss: 1.7925516137709985 MRPC Val Loss: 0.8228422827445544\n",
            " RTE Val Acc: 0.6069711538461539 MRPC Val Acc: 0.8100961538461539\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [02:36<00:00,  1.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21 RTE Train Loss: 0.03814011543770047\n",
            "Epoch 21 RTE Train Loss: 0.042844594383210526 MRPC Train Loss: 0.03149017666046526\n",
            " RTE Train Acc: 0.9966299019607843 MRPC Train Acc: 0.9984681372549019\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:11<00:00,  4.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21 RTE Val Loss: 1.7734615665215712 MRPC Val Loss: 0.8552898317575455\n",
            " RTE Val Acc: 0.5877403846153846 MRPC Val Acc: 0.8137019230769231\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [02:35<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22 RTE Train Loss: 0.03487647985856907\n",
            "Epoch 22 RTE Train Loss: 0.04189883546867207 MRPC Train Loss: 0.029379229043044297\n",
            " RTE Train Acc: 0.9963235294117647 MRPC Train Acc: 0.9987745098039216\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:11<00:00,  4.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22 RTE Val Loss: 1.9791143765816321 MRPC Val Loss: 0.8902408560881248\n",
            " RTE Val Acc: 0.5817307692307693 MRPC Val Acc: 0.8149038461538461\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [02:36<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23 RTE Train Loss: 0.03503313463400392\n",
            "Epoch 23 RTE Train Loss: 0.03975444730809506 MRPC Train Loss: 0.028529804528636092\n",
            " RTE Train Acc: 0.9966299019607843 MRPC Train Acc: 0.9990808823529411\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:11<00:00,  4.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23 RTE Val Loss: 2.0099148887854357 MRPC Val Loss: 1.005824858179459\n",
            " RTE Val Acc: 0.5540865384615384 MRPC Val Acc: 0.7920673076923077\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [02:36<00:00,  1.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24 RTE Train Loss: 0.05497219873701825\n",
            "Epoch 24 RTE Train Loss: 0.057839512405003984 MRPC Train Loss: 0.03999195596678\n",
            " RTE Train Acc: 0.991421568627451 MRPC Train Acc: 0.9954044117647058\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:11<00:00,  4.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24 RTE Val Loss: 1.8575240396536314 MRPC Val Loss: 0.8185006941740329\n",
            " RTE Val Acc: 0.5745192307692307 MRPC Val Acc: 0.8125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [02:35<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25 RTE Train Loss: 0.039201401557554215\n",
            "Epoch 25 RTE Train Loss: 0.04433729349836415 MRPC Train Loss: 0.03264918141797477\n",
            " RTE Train Acc: 0.9954044117647058 MRPC Train Acc: 0.9975490196078431\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:11<00:00,  4.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25 RTE Val Loss: 2.0735681698872495 MRPC Val Loss: 0.8656775040122179\n",
            " RTE Val Acc: 0.5697115384615384 MRPC Val Acc: 0.8161057692307693\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [02:35<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26 RTE Train Loss: 0.03171266220948275\n",
            "Epoch 26 RTE Train Loss: 0.036812761013268254 MRPC Train Loss: 0.03255352920249981\n",
            " RTE Train Acc: 0.9960171568627451 MRPC Train Acc: 0.9969362745098039\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:11<00:00,  4.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26 RTE Val Loss: 2.192609135921185 MRPC Val Loss: 0.979788064956665\n",
            " RTE Val Acc: 0.5673076923076923 MRPC Val Acc: 0.8064903846153846\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [02:36<00:00,  1.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27 RTE Train Loss: 0.040454098151302804\n",
            "Epoch 27 RTE Train Loss: 0.04144724781679757 MRPC Train Loss: 0.030573452165459886\n",
            " RTE Train Acc: 0.9944852941176471 MRPC Train Acc: 0.9978553921568627\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:11<00:00,  4.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27 RTE Val Loss: 2.0282063025694628 MRPC Val Loss: 0.8656305854137127\n",
            " RTE Val Acc: 0.5865384615384616 MRPC Val Acc: 0.8245192307692307\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [02:36<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28 RTE Train Loss: 0.03111024902147405\n",
            "Epoch 28 RTE Train Loss: 0.036074319525676614 MRPC Train Loss: 0.025435626305931925\n",
            " RTE Train Acc: 0.9963235294117647 MRPC Train Acc: 0.9987745098039216\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:11<00:00,  4.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28 RTE Val Loss: 2.0992742501772366 MRPC Val Loss: 0.8698627146390768\n",
            " RTE Val Acc: 0.5913461538461539 MRPC Val Acc: 0.8173076923076923\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [02:35<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29 RTE Train Loss: 0.03039208483681375\n",
            "Epoch 29 RTE Train Loss: 0.03701201446500479 MRPC Train Loss: 0.024275266912345793\n",
            " RTE Train Acc: 0.9966299019607843 MRPC Train Acc: 0.9987745098039216\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:11<00:00,  4.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29 RTE Val Loss: 1.9868195698811457 MRPC Val Loss: 0.9273501015626467\n",
            " RTE Val Acc: 0.5841346153846154 MRPC Val Acc: 0.8149038461538461\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [02:36<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30 RTE Train Loss: 0.030268624735375244\n",
            "Epoch 30 RTE Train Loss: 0.0352618250557605 MRPC Train Loss: 0.023883143202493004\n",
            " RTE Train Acc: 0.9960171568627451 MRPC Train Acc: 0.9987745098039216\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:11<00:00,  4.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30 RTE Val Loss: 1.9482103219399085 MRPC Val Loss: 0.9542087625998718\n",
            " RTE Val Acc: 0.5600961538461539 MRPC Val Acc: 0.8100961538461539\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [02:36<00:00,  1.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31 RTE Train Loss: 0.024988239466705742\n",
            "Epoch 31 RTE Train Loss: 0.0331965810162764 MRPC Train Loss: 0.021180593773868737\n",
            " RTE Train Acc: 0.9963235294117647 MRPC Train Acc: 0.9996936274509803\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:11<00:00,  4.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31 RTE Val Loss: 2.087399083834428 MRPC Val Loss: 0.9704800305458215\n",
            " RTE Val Acc: 0.5709134615384616 MRPC Val Acc: 0.8161057692307693\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [02:35<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32 RTE Train Loss: 0.02559958964440168\n",
            "Epoch 32 RTE Train Loss: 0.030063839537986352 MRPC Train Loss: 0.02610739196340243\n",
            " RTE Train Acc: 0.9972426470588235 MRPC Train Acc: 0.9975490196078431\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:11<00:00,  4.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32 RTE Val Loss: 2.13545672251628 MRPC Val Loss: 0.9862274000277886\n",
            " RTE Val Acc: 0.5829326923076923 MRPC Val Acc: 0.8088942307692307\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [02:36<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 33 RTE Train Loss: 0.023228620781618005\n",
            "Epoch 33 RTE Train Loss: 0.03027063792607948 MRPC Train Loss: 0.018656336483271682\n",
            " RTE Train Acc: 0.9981617647058824 MRPC Train Acc: 0.9996936274509803\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:11<00:00,  4.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 33 RTE Val Loss: 2.1365135678878198 MRPC Val Loss: 1.0009046632509966\n",
            " RTE Val Acc: 0.5865384615384616 MRPC Val Acc: 0.8088942307692307\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [02:35<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 34 RTE Train Loss: 0.02629187969746543\n",
            "Epoch 34 RTE Train Loss: 0.027696683455039475 MRPC Train Loss: 0.02296027500072823\n",
            " RTE Train Acc: 0.9969362745098039 MRPC Train Acc: 0.9984681372549019\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:11<00:00,  4.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 34 RTE Val Loss: 2.1587123412352343 MRPC Val Loss: 1.1069676517867124\n",
            " RTE Val Acc: 0.5733173076923077 MRPC Val Acc: 0.8004807692307693\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [02:36<00:00,  1.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 35 RTE Train Loss: 0.023527873259987318\n",
            "Epoch 35 RTE Train Loss: 0.02671876186322348 MRPC Train Loss: 0.018875173868282753\n",
            " RTE Train Acc: 0.9978553921568627 MRPC Train Acc: 0.9996936274509803\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:11<00:00,  4.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 35 RTE Val Loss: 2.2501260638237 MRPC Val Loss: 1.042746053292201\n",
            " RTE Val Acc: 0.5745192307692307 MRPC Val Acc: 0.8100961538461539\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [02:36<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 36 RTE Train Loss: 0.027020877062836113\n",
            "Epoch 36 RTE Train Loss: 0.028211329442759354 MRPC Train Loss: 0.018942791210743142\n",
            " RTE Train Acc: 0.9975490196078431 MRPC Train Acc: 0.9996936274509803\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:11<00:00,  4.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 36 RTE Val Loss: 2.086038291454315 MRPC Val Loss: 0.9809170628969486\n",
            " RTE Val Acc: 0.5925480769230769 MRPC Val Acc: 0.8052884615384616\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [02:36<00:00,  1.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 37 RTE Train Loss: 0.027148513105131833\n",
            "Epoch 37 RTE Train Loss: 0.03357361888914716 MRPC Train Loss: 0.02433218818851838\n",
            " RTE Train Acc: 0.9966299019607843 MRPC Train Acc: 0.9984681372549019\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:11<00:00,  4.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 37 RTE Val Loss: 2.0185172649530263 MRPC Val Loss: 0.9820048946600693\n",
            " RTE Val Acc: 0.5733173076923077 MRPC Val Acc: 0.8052884615384616\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [02:35<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 38 RTE Train Loss: 0.02377627968934237\n",
            "Epoch 38 RTE Train Loss: 0.030037618223942963 MRPC Train Loss: 0.0173478056128849\n",
            " RTE Train Acc: 0.9981617647058824 MRPC Train Acc: 0.9993872549019608\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:11<00:00,  4.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 38 RTE Val Loss: 2.169357414429004 MRPC Val Loss: 1.0427813805066621\n",
            " RTE Val Acc: 0.5733173076923077 MRPC Val Acc: 0.8052884615384616\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [02:36<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 39 RTE Train Loss: 0.024193248273256945\n",
            "Epoch 39 RTE Train Loss: 0.02713756215776883 MRPC Train Loss: 0.018168647225727055\n",
            " RTE Train Acc: 0.9981617647058824 MRPC Train Acc: 0.9996936274509803\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:11<00:00,  4.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 39 RTE Val Loss: 2.2821176097943234 MRPC Val Loss: 1.0495719250578146\n",
            " RTE Val Acc: 0.5661057692307693 MRPC Val Acc: 0.8137019230769231\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [02:35<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40 RTE Train Loss: 0.024141136502080104\n",
            "Epoch 40 RTE Train Loss: 0.02834867200284612 MRPC Train Loss: 0.018877933304026432\n",
            " RTE Train Acc: 0.9978553921568627 MRPC Train Acc: 0.9993872549019608\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:11<00:00,  4.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40 RTE Val Loss: 2.276034735716306 MRPC Val Loss: 0.9155649049923971\n",
            " RTE Val Acc: 0.5516826923076923 MRPC Val Acc: 0.7908653846153846\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [02:35<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 41 RTE Train Loss: 0.03937327158728651\n",
            "Epoch 41 RTE Train Loss: 0.05034770880478854 MRPC Train Loss: 0.022561430264556526\n",
            " RTE Train Acc: 0.9911151960784313 MRPC Train Acc: 0.9978553921568627\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:11<00:00,  4.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 41 RTE Val Loss: 2.202215749483842 MRPC Val Loss: 1.0182206905805147\n",
            " RTE Val Acc: 0.5805288461538461 MRPC Val Acc: 0.796875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [02:36<00:00,  1.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 42 RTE Train Loss: 0.03594031117345188\n",
            "Epoch 42 RTE Train Loss: 0.040513537130227276 MRPC Train Loss: 0.021635152079968478\n",
            " RTE Train Acc: 0.9941789215686274 MRPC Train Acc: 0.9975490196078431\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:11<00:00,  4.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 42 RTE Val Loss: 2.1295881867408752 MRPC Val Loss: 1.0388510502301729\n",
            " RTE Val Acc: 0.5540865384615384 MRPC Val Acc: 0.7992788461538461\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [02:36<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 43 RTE Train Loss: 0.04275473083058993\n",
            "Epoch 43 RTE Train Loss: 0.052215232641673555 MRPC Train Loss: 0.02998684438895069\n",
            " RTE Train Acc: 0.9917279411764706 MRPC Train Acc: 0.9963235294117647\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:12<00:00,  4.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 43 RTE Val Loss: 1.9407893602664654 MRPC Val Loss: 1.0959688803324332\n",
            " RTE Val Acc: 0.5721153846153846 MRPC Val Acc: 0.7908653846153846\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [02:35<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 44 RTE Train Loss: 0.03898981803407272\n",
            "Epoch 44 RTE Train Loss: 0.03885186786818154 MRPC Train Loss: 0.032272954526193\n",
            " RTE Train Acc: 0.9944852941176471 MRPC Train Acc: 0.9963235294117647\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:11<00:00,  4.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 44 RTE Val Loss: 2.240685293307671 MRPC Val Loss: 0.9123702656764251\n",
            " RTE Val Acc: 0.5600961538461539 MRPC Val Acc: 0.8112980769230769\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [02:36<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 45 RTE Train Loss: 0.02635776327338581\n",
            "Epoch 45 RTE Train Loss: 0.02899842884610681 MRPC Train Loss: 0.01923426159420142\n",
            " RTE Train Acc: 0.9975490196078431 MRPC Train Acc: 0.9990808823529411\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:11<00:00,  4.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 45 RTE Val Loss: 2.272726778800671 MRPC Val Loss: 0.9511295247536439\n",
            " RTE Val Acc: 0.5552884615384616 MRPC Val Acc: 0.8100961538461539\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [02:36<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 46 RTE Train Loss: 0.020515319736053545\n",
            "Epoch 46 RTE Train Loss: 0.02493820417964575 MRPC Train Loss: 0.015276786734295241\n",
            " RTE Train Acc: 0.9978553921568627 MRPC Train Acc: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:11<00:00,  4.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 46 RTE Val Loss: 2.431387882966262 MRPC Val Loss: 1.0707403020216868\n",
            " RTE Val Acc: 0.5661057692307693 MRPC Val Acc: 0.8040865384615384\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [02:35<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 47 RTE Train Loss: 0.020209636110081978\n",
            "Epoch 47 RTE Train Loss: 0.02359288790281497 MRPC Train Loss: 0.014894064324086203\n",
            " RTE Train Acc: 0.9984681372549019 MRPC Train Acc: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:11<00:00,  4.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 47 RTE Val Loss: 2.4266158525760355 MRPC Val Loss: 1.1202301933215215\n",
            " RTE Val Acc: 0.5637019230769231 MRPC Val Acc: 0.8125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [02:35<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 48 RTE Train Loss: 0.019751323377896174\n",
            "Epoch 48 RTE Train Loss: 0.025105955060936658 MRPC Train Loss: 0.016330570922981874\n",
            " RTE Train Acc: 0.9987745098039216 MRPC Train Acc: 0.9996936274509803\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:11<00:00,  4.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 48 RTE Val Loss: 2.3671939235467176 MRPC Val Loss: 1.110929820400018\n",
            " RTE Val Acc: 0.5420673076923077 MRPC Val Acc: 0.7896634615384616\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [02:36<00:00,  1.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 49 RTE Train Loss: 0.017654120392513042\n",
            "Epoch 49 RTE Train Loss: 0.02157626954802111 MRPC Train Loss: 0.014236512876974017\n",
            " RTE Train Acc: 0.9981617647058824 MRPC Train Acc: 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:11<00:00,  4.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 49 RTE Val Loss: 2.465914263175084 MRPC Val Loss: 1.1337732684153776\n",
            " RTE Val Acc: 0.5685096153846154 MRPC Val Acc: 0.8100961538461539\n",
            "Total_train_loss :  [2.50285329655105, 1.3948067999353595, 1.3743851886076086, 1.2259191227894204, 0.9953354071168339, 0.7460764257931242, 0.48081077869031946, 0.3694872302459736, 0.2937239491764237, 0.2329426313586095, 0.21773897987954757, 0.19172514785154193, 0.17087052725985938, 0.15835230855965146, 0.13013801526497393, 0.12219441302266776, 0.14858603294865758, 0.11508623561730572, 0.1173400365400548, 0.09982032989900486, 0.08757957845341925, 0.07628023087540094, 0.06975295971713814, 0.07006626926800784, 0.1099443974740365, 0.07840280311510843, 0.0634253244189655, 0.08090819630260561, 0.0622204980429481, 0.0607841696736275, 0.06053724947075049, 0.049976478933411485, 0.05119917928880336, 0.04645724156323601, 0.05258375939493086, 0.047055746519974635, 0.05404175412567223, 0.054297026210263666, 0.04755255937868474, 0.04838649654651389, 0.04828227300416021, 0.07874654317457302, 0.07188062234690376, 0.08550946166117986, 0.07797963606814544, 0.05271552654677162, 0.04103063947210709, 0.040419272220163956, 0.03950264675579235, 0.035308240785026083]\n",
            "RTE Train Loss:  [1.334515515495749, 0.7292657798411799, 0.7284209564620373, 0.7134610239197227, 0.6715581770621094, 0.5394690296813553, 0.3419480626197422, 0.23742316823964024, 0.18098012233773866, 0.14024771932585567, 0.13493443656639725, 0.11734684322978936, 0.10482775595258265, 0.09405309553532039, 0.07923773303627968, 0.06884033557977162, 0.08767319247857028, 0.0644712114670113, 0.06295476539754401, 0.056782796248501424, 0.04572204270345323, 0.042844594383210526, 0.04189883546867207, 0.03975444730809506, 0.057839512405003984, 0.04433729349836415, 0.036812761013268254, 0.04144724781679757, 0.036074319525676614, 0.03701201446500479, 0.0352618250557605, 0.0331965810162764, 0.030063839537986352, 0.03027063792607948, 0.027696683455039475, 0.02671876186322348, 0.028211329442759354, 0.03357361888914716, 0.030037618223942963, 0.02713756215776883, 0.02834867200284612, 0.05034770880478854, 0.040513537130227276, 0.052215232641673555, 0.03885186786818154, 0.02899842884610681, 0.02493820417964575, 0.02359288790281497, 0.025105955060936658, 0.02157626954802111]\n",
            "MRPC Train Loss:  [1.149019192831189, 0.66376651560559, 0.6338868497633466, 0.46655357935849356, 0.26490774845667914, 0.17951090772654496, 0.12408912609166958, 0.10897611015859772, 0.09944964302521125, 0.08129944820322242, 0.07483405375159254, 0.0657260703543822, 0.062140238270455714, 0.0629663849797319, 0.05648158203956543, 0.05395722995493926, 0.05786482199076928, 0.04655566040937807, 0.047750954917979004, 0.042858637303260026, 0.039806607562829464, 0.03149017666046526, 0.029379229043044297, 0.028529804528636092, 0.03999195596678, 0.03264918141797477, 0.03255352920249981, 0.030573452165459886, 0.025435626305931925, 0.024275266912345793, 0.023883143202493004, 0.021180593773868737, 0.02610739196340243, 0.018656336483271682, 0.02296027500072823, 0.018875173868282753, 0.018942791210743142, 0.02433218818851838, 0.0173478056128849, 0.018168647225727055, 0.018877933304026432, 0.022561430264556526, 0.021635152079968478, 0.02998684438895069, 0.032272954526193, 0.01923426159420142, 0.015276786734295241, 0.014894064324086203, 0.016330570922981874, 0.014236512876974017]\n",
            "RTE Dev Loss:  [0.7320397129425635, 0.7169663401750418, 0.7105068060067984, 0.7115780550699967, 0.6987266701001388, 0.918610882300597, 1.1552517459942744, 1.1517417362103095, 1.2255823795612042, 1.3267897711350367, 1.2916511595249176, 1.3559237512258382, 1.4887432914513807, 1.3826494148144355, 1.7487884163856506, 1.7620424261459937, 1.547631025314331, 1.6331771153670092, 1.660010617512923, 1.6736480731230516, 1.7925516137709985, 1.7734615665215712, 1.9791143765816321, 2.0099148887854357, 1.8575240396536314, 2.0735681698872495, 2.192609135921185, 2.0282063025694628, 2.0992742501772366, 1.9868195698811457, 1.9482103219399085, 2.087399083834428, 2.13545672251628, 2.1365135678878198, 2.1587123412352343, 2.2501260638237, 2.086038291454315, 2.0185172649530263, 2.169357414429004, 2.2821176097943234, 2.276034735716306, 2.202215749483842, 2.1295881867408752, 1.9407893602664654, 2.240685293307671, 2.272726778800671, 2.431387882966262, 2.4266158525760355, 2.3671939235467176, 2.465914263175084]\n",
            "MRPC Dev Loss:  [0.6699261138072381, 0.6410394987234702, 0.5944715887308121, 0.48722034578139967, 0.4553659546833772, 0.5434417569866548, 0.5719578495392432, 0.608026860998227, 0.6317977561400487, 0.6382897215393873, 0.6520576041478378, 0.7373727170320657, 0.7514303567317816, 0.740893294604925, 0.7638280277068799, 0.7328039178481469, 0.6707254017774875, 0.6741555447761829, 0.7625342435561694, 0.8446574004796835, 0.8228422827445544, 0.8552898317575455, 0.8902408560881248, 1.005824858179459, 0.8185006941740329, 0.8656775040122179, 0.979788064956665, 0.8656305854137127, 0.8698627146390768, 0.9273501015626467, 0.9542087625998718, 0.9704800305458215, 0.9862274000277886, 1.0009046632509966, 1.1069676517867124, 1.042746053292201, 0.9809170628969486, 0.9820048946600693, 1.0427813805066621, 1.0495719250578146, 0.9155649049923971, 1.0182206905805147, 1.0388510502301729, 1.0959688803324332, 0.9123702656764251, 0.9511295247536439, 1.0707403020216868, 1.1202301933215215, 1.110929820400018, 1.1337732684153776]\n",
            "RTE Train Acc:  [0.49356617647058826, 0.4806985294117647, 0.49019607843137253, 0.5208333333333334, 0.6259191176470589, 0.7775735294117647, 0.8863357843137255, 0.9390318627450981, 0.9620098039215687, 0.9742647058823529, 0.9751838235294118, 0.9813112745098039, 0.9837622549019608, 0.984375, 0.9874387254901961, 0.9883578431372549, 0.9856004901960784, 0.9926470588235294, 0.9917279411764706, 0.9932598039215687, 0.9957107843137255, 0.9966299019607843, 0.9963235294117647, 0.9966299019607843, 0.991421568627451, 0.9954044117647058, 0.9960171568627451, 0.9944852941176471, 0.9963235294117647, 0.9966299019607843, 0.9960171568627451, 0.9963235294117647, 0.9972426470588235, 0.9981617647058824, 0.9969362745098039, 0.9978553921568627, 0.9975490196078431, 0.9966299019607843, 0.9981617647058824, 0.9981617647058824, 0.9978553921568627, 0.9911151960784313, 0.9941789215686274, 0.9917279411764706, 0.9944852941176471, 0.9975490196078431, 0.9978553921568627, 0.9984681372549019, 0.9987745098039216, 0.9981617647058824]\n",
            "MRPC Train Acc:  [0.6617647058823529, 0.6715686274509803, 0.6801470588235294, 0.8207720588235294, 0.9283088235294118, 0.9635416666666666, 0.9816176470588235, 0.9840686274509803, 0.9846813725490197, 0.991421568627451, 0.9920343137254902, 0.9932598039215687, 0.9944852941176471, 0.9932598039215687, 0.9947916666666666, 0.9944852941176471, 0.992953431372549, 0.9947916666666666, 0.9947916666666666, 0.9957107843137255, 0.9960171568627451, 0.9984681372549019, 0.9987745098039216, 0.9990808823529411, 0.9954044117647058, 0.9975490196078431, 0.9969362745098039, 0.9978553921568627, 0.9987745098039216, 0.9987745098039216, 0.9987745098039216, 0.9996936274509803, 0.9975490196078431, 0.9996936274509803, 0.9984681372549019, 0.9996936274509803, 0.9996936274509803, 0.9984681372549019, 0.9993872549019608, 0.9996936274509803, 0.9993872549019608, 0.9978553921568627, 0.9975490196078431, 0.9963235294117647, 0.9963235294117647, 0.9990808823529411, 1.0, 1.0, 0.9996936274509803, 1.0]\n",
            "RTE Dev Acc:  [0.49399038461538464, 0.4963942307692308, 0.5420673076923077, 0.49158653846153844, 0.6213942307692307, 0.5865384615384616, 0.5480769230769231, 0.6021634615384616, 0.5985576923076923, 0.5973557692307693, 0.5985576923076923, 0.6009615384615384, 0.6021634615384616, 0.6153846153846154, 0.5877403846153846, 0.5889423076923077, 0.5913461538461539, 0.5865384615384616, 0.5757211538461539, 0.59375, 0.6069711538461539, 0.5877403846153846, 0.5817307692307693, 0.5540865384615384, 0.5745192307692307, 0.5697115384615384, 0.5673076923076923, 0.5865384615384616, 0.5913461538461539, 0.5841346153846154, 0.5600961538461539, 0.5709134615384616, 0.5829326923076923, 0.5865384615384616, 0.5733173076923077, 0.5745192307692307, 0.5925480769230769, 0.5733173076923077, 0.5733173076923077, 0.5661057692307693, 0.5516826923076923, 0.5805288461538461, 0.5540865384615384, 0.5721153846153846, 0.5600961538461539, 0.5552884615384616, 0.5661057692307693, 0.5637019230769231, 0.5420673076923077, 0.5685096153846154]\n",
            "MRPC Dev Acc:  [0.6899038461538461, 0.6911057692307693, 0.6947115384615384, 0.7932692307692307, 0.8149038461538461, 0.8173076923076923, 0.8245192307692307, 0.8161057692307693, 0.8125, 0.8137019230769231, 0.8245192307692307, 0.8052884615384616, 0.8149038461538461, 0.8040865384615384, 0.8052884615384616, 0.8100961538461539, 0.8161057692307693, 0.8245192307692307, 0.8245192307692307, 0.8016826923076923, 0.8100961538461539, 0.8137019230769231, 0.8149038461538461, 0.7920673076923077, 0.8125, 0.8161057692307693, 0.8064903846153846, 0.8245192307692307, 0.8173076923076923, 0.8149038461538461, 0.8100961538461539, 0.8161057692307693, 0.8088942307692307, 0.8088942307692307, 0.8004807692307693, 0.8100961538461539, 0.8052884615384616, 0.8052884615384616, 0.8052884615384616, 0.8137019230769231, 0.7908653846153846, 0.796875, 0.7992788461538461, 0.7908653846153846, 0.8112980769230769, 0.8100961538461539, 0.8040865384615384, 0.8125, 0.7896634615384616, 0.8100961538461539]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "beta 1e-03, epochs 10\n",
        "Total_train_loss :  [2.1569354136784873, 1.2927938074457879, 1.1335661177541696, 0.9108231540988473, 0.6455930538621604, 0.46185997420666264, 0.35249851556385264, 0.31616470831282, 0.28938400000333786, 0.22912178940924943]\n",
        "RTE Train Loss:  [1.1048656228710623, 0.6840196859602835, 0.6065010638797984, 0.49441171715072557, 0.3303750535728885, 0.22947271102491548, 0.17925063075095998, 0.15636680596599392, 0.14810886781881838, 0.11634161586270612]\n",
        "MRPC Train Loss:  [1.033039130416571, 0.5938807597347334, 0.506886244988909, 0.39245667337786916, 0.2799694681284474, 0.2109561374082285, 0.16830027979962967, 0.1404654908121801, 0.12685256575544676, 0.10973908222627406]\n",
        "RTE Dev Loss:  [0.7373840900567862, 0.8249692297898806, 0.7162189483642578, 0.8412233247206762, 0.9673777130933908, 1.0642799161947691, 1.274667888879776, 1.1392017465371351, 1.3254191394035633, 1.4177790398781116]\n",
        "MRPC Dev Loss:  [0.6544087781355932, 0.6297953587311965, 0.5568436017403235, 0.582781576193296, 0.6071517616510391, 0.6568600970965165, 0.7656649901316717, 0.7258225553310834, 0.7412626227507224, 0.7365222435731155]\n",
        "RTE Train Acc:  [0.5058210784313726, 0.6115196078431373, 0.7025122549019608, 0.7984068627450981, 0.8949142156862745, 0.9430147058823529, 0.9650735294117647, 0.9718137254901961, 0.9708946078431373, 0.9834558823529411]\n",
        "MRPC Train Acc:  [0.6660539215686274, 0.7224264705882353, 0.8048406862745098, 0.8624387254901961, 0.9231004901960784, 0.9515931372549019, 0.9666053921568627, 0.975796568627451, 0.9800857843137255, 0.984375]\n",
        "RTE Dev Acc:  [0.4951923076923077, 0.5865384615384616, 0.6346153846153846, 0.5877403846153846, 0.6189903846153846, 0.6105769230769231, 0.6117788461538461, 0.6394230769230769, 0.5913461538461539, 0.578125]\n",
        "MRPC Dev Acc:  [0.6935096153846154, 0.7043269230769231, 0.7584134615384616, 0.7379807692307693, 0.765625, 0.765625, 0.7584134615384616, 0.7728365384615384, 0.7740384615384616, 0.7860576923076923]\n",
        "\n",
        "Epoch 1 RTE Test Loss: 1.4761454926596747 MRPC Test Loss: 0.7835625788340201\n",
        " RTE Test Acc: 0.5891203703703703 MRPC Test Acc: 0.7776442307692307\n",
        " RTE Test F1: 0.5842723926370899 MRPC Test F1: 0.836523252732936\n",
        "\n",
        "Epoch 1 RTE Test Loss: 0.0 MRPC Test Loss: 0.7878316282122223\n",
        " RTE Test Acc: 0.0 MRPC Test Acc: 0.7219228927203066\n",
        " RTE Test F1: 0.0 MRPC Test F1: 0.7963072599011966\n",
        "\n",
        "beta 1e-04 epochs 10\n",
        "Total_train_loss :  [1.4782134528253592, 1.0491686726317686, 0.5989378326955963, 0.2932136272098504, 0.1783044737051515, 0.14107874138098137, 0.09611262110810653, 0.10848711288588889, 0.07655961714758008, 0.0645764537301718]\n",
        "RTE Train Loss:  [0.7750658556526783, 0.5839595137273564, 0.3623182758981106, 0.18668376771258374, 0.11544408676598002, 0.08070473920773058, 0.058435393168645745, 0.06003689840801206, 0.040856739999178576, 0.041136031075581614]\n",
        "MRPC Train Loss:  [0.6888802857959971, 0.4125157302501155, 0.18832684074546777, 0.08987735615422328, 0.0581227570370424, 0.048829952017495445, 0.03593965754935555, 0.03365324404748047, 0.02688894014549898, 0.021642265101785168]\n",
        "RTE Dev Loss:  [0.6855802169212928, 0.7480734082368704, 0.8674726807154142, 0.9842896335400068, 1.1043801170129042, 1.140833162344419, 1.3175011506447425, 1.3092539929426634, 1.402744316137754, 1.3269579250078936]\n",
        "MRPC Dev Loss:  [0.5314731712524707, 0.3557314259501604, 0.4846974473733168, 0.5817678479047922, 0.5850505719964321, 0.5742644340946124, 0.7346780655475763, 0.72365936006491, 0.7643713681743696, 0.6945428596093104]\n",
        "RTE Train Acc:  [0.5229779411764706, 0.71875, 0.8673406862745098, 0.9482230392156863, 0.9708946078431373, 0.9831495098039216, 0.9908088235294118, 0.9874387254901961, 0.992953431372549, 0.9941789215686274]\n",
        "MRPC Train Acc:  [0.6798406862745098, 0.8370098039215687, 0.9525122549019608, 0.9819240196078431, 0.9901960784313726, 0.9932598039215687, 0.992953431372549, 0.9947916666666666, 0.9975490196078431, 0.9975490196078431]\n",
        "RTE Dev Acc:  [0.6418269230769231, 0.6418269230769231, 0.6418269230769231, 0.6622596153846154, 0.6682692307692307, 0.6634615384615384, 0.6586538461538461, 0.6706730769230769, 0.6646634615384616, 0.6790865384615384]\n",
        "MRPC Dev Acc:  [0.7836538461538461, 0.8641826923076923, 0.8088942307692307, 0.8317307692307693, 0.8353365384615384, 0.8329326923076923, 0.8233173076923077, 0.8173076923076923, 0.8245192307692307, 0.828125]\n",
        "\n",
        "Epoch 1 RTE Test Loss: 1.280003062000981 MRPC Test Loss: 0.6871244019040694\n",
        " RTE Test Acc: 0.6527777777777778 MRPC Test Acc: 0.828125\n",
        " RTE Test F1: 0.6436523232285588 MRPC Test F1: 0.8765453009343809\n",
        " \n",
        " Epoch 1 RTE Test Loss: 0.0 MRPC Test Loss: 0.6950681506207695\n",
        " RTE Test Acc: 0.0 MRPC Test Acc: 0.8284642401021711\n",
        " RTE Test F1: 0.0 MRPC Test F1: 0.8721314473475146\n",
        " \n",
        " beta 1e-02 epochs 10\n",
        " RTE Train Loss:  [5.368059420118145, 0.7368292346888897, 0.7307087416742363, 0.7286782551045511, 0.7253740716214273, 0.7246266977459777, 0.7235575668952045, 0.7206465800603231, 0.7149606136714711, 0.6681692424942466]\n",
        "MRPC Train Loss:  [5.119580800042433, 0.6726018985112509, 0.6665304869997735, 0.6655615156187731, 0.6646054927624908, 0.6620761454105377, 0.6602996383227554, 0.653288815535751, 0.6276463652942695, 0.5309239102929246]\n",
        "RTE Dev Loss:  [0.742461812037688, 0.7116771730092856, 0.7180657707727872, 0.7197485107641953, 0.7134129634270301, 0.7138146116183355, 0.7096136097724621, 0.7078442917420313, 0.7022165885338416, 0.6986170777907739]\n",
        "MRPC Dev Loss:  [0.6690539992772616, 0.6337411105632782, 0.6356711341784551, 0.6389378240475287, 0.6407323708901038, 0.6371294076626117, 0.6385073891052833, 0.6235784750718337, 0.5913414164231374, 0.5750442823538413]\n",
        "RTE Train Acc:  [0.4987745098039216, 0.5036764705882353, 0.4950980392156863, 0.49203431372549017, 0.5073529411764706, 0.48651960784313725, 0.49080882352941174, 0.4990808823529412, 0.5481004901960784, 0.7349877450980392]\n",
        "MRPC Train Acc:  [0.6173406862745098, 0.6715686274509803, 0.6703431372549019, 0.6709558823529411, 0.6709558823529411, 0.6703431372549019, 0.6709558823529411, 0.6712622549019608, 0.6712622549019608, 0.7901348039215687]\n",
        "RTE Dev Acc:  [0.5024038461538461, 0.5324519230769231, 0.5108173076923077, 0.47475961538461536, 0.4951923076923077, 0.5084134615384616, 0.5276442307692307, 0.5240384615384616, 0.5745192307692307, 0.6286057692307693]\n",
        "MRPC Dev Acc:  [0.6923076923076923, 0.6959134615384616, 0.6923076923076923, 0.6923076923076923, 0.6947115384615384, 0.6923076923076923, 0.6923076923076923, 0.6935096153846154, 0.6875, 0.7776442307692307]\n",
        "\n",
        "beta 1e-05 epochs 10\n",
        "Total_train_loss :  [1.3418563382298339, 0.960776573302699, 0.554785922461865, 0.27949002114873306, 0.1411113846207074, 0.08485924196886081, 0.048074360608178025, 0.04682943707003313, 0.03249919707612956, 0.025782232154525964]\n",
        "RTE Train Loss:  [0.7131768596522948, 0.5282030713324454, 0.33293822889818864, 0.1681957293082686, 0.09704998257916932, 0.05419483540725766, 0.03459611751924397, 0.03142696123698032, 0.02254222953399899, 0.01922451831199521]\n",
        "MRPC Train Loss:  [0.6060559393144122, 0.38502678421198155, 0.18585141225918836, 0.07949976579231374, 0.033546898011848625, 0.026247976051059132, 0.013182961701543308, 0.013779357075691223, 0.007234672650548757, 0.011661303202238153]\n",
        "RTE Dev Loss:  [0.7009723690839914, 0.7610760846963296, 0.9045548370251288, 0.9844953142679654, 1.1329587537508745, 1.097912916770348, 1.4755209340498998, 1.2231850715783925, 1.6219298128898327, 1.5469015332368703]\n",
        "MRPC Dev Loss:  [0.5634498355480341, 0.4716861511652286, 0.5179019788136849, 0.56533765907471, 0.7992479107700862, 0.7358048328986535, 0.8984657881351618, 0.7793938420139827, 1.0537244860942547, 0.9647668623007261]\n",
        "RTE Train Acc:  [0.5533088235294118, 0.7521446078431373, 0.8707107843137255, 0.9439338235294118, 0.9745710784313726, 0.9856004901960784, 0.9898897058823529, 0.9905024509803921, 0.9954044117647058, 0.9947916666666666]\n",
        "MRPC Train Acc:  [0.7015931372549019, 0.8621323529411765, 0.9430147058823529, 0.9794730392156863, 0.9941789215686274, 0.9938725490196079, 0.9984681372549019, 0.9984681372549019, 0.9990808823529411, 0.9975490196078431]\n",
        "RTE Dev Acc:  [0.5853365384615384, 0.6598557692307693, 0.6574519230769231, 0.6586538461538461, 0.6923076923076923, 0.6790865384615384, 0.6826923076923077, 0.6706730769230769, 0.6730769230769231, 0.6899038461538461]\n",
        "MRPC Dev Acc:  [0.7596153846153846, 0.7980769230769231, 0.8137019230769231, 0.8209134615384616, 0.8173076923076923, 0.8185096153846154, 0.8076923076923077, 0.8185096153846154, 0.8125, 0.8137019230769231]\n",
        "\n",
        "Epoch 1 RTE Test Loss: 1.4891086596029777 MRPC Test Loss: 0.9578616991639137\n",
        " RTE Test Acc: 0.6631944444444444 MRPC Test Acc: 0.8173076923076923\n",
        " RTE Test F1: 0.5984077901748426 MRPC Test F1: 0.8701315157000705\n",
        "\n",
        "Epoch 1 RTE Test Loss: 0.0 MRPC Test Loss: 0.9600973040624349\n",
        " RTE Test Acc: 0.0 MRPC Test Acc: 0.8210009578544061\n",
        " RTE Test F1: 0.0 MRPC Test F1: 0.8677421848845047\n",
        "\n",
        "beta 1e-06 epochs 10\n",
        "Epoch 1 RTE Test Loss: 1.7219745627156011 MRPC Test Loss: 0.8393962348882968\n",
        " RTE Test Acc: 0.6435185185185185 MRPC Test Acc: 0.8329326923076923\n",
        " RTE Test F1: 0.5942014728703208 MRPC Test F1: 0.8818459425720597\n",
        "\n",
        "Epoch 1 RTE Test Loss: 0.0 MRPC Test Loss: 0.8770330124707134\n",
        " RTE Test Acc: 0.0 MRPC Test Acc: 0.823315772669221\n",
        " RTE Test F1: 0.0 MRPC Test F1: 0.8689215978379984\n",
        "\n",
        " Total_train_loss :  [1.2759807962997287, 0.8335171485064077, 0.4170333024333505, 0.15028686028010413, 0.0708995140203293, 0.05041659640732641, 0.036466566339025605, 0.0460747832651524, 0.02340566739896495, 0.018694454237964807]\n",
        "RTE Train Loss:  [0.6754848831424526, 0.46001824561287374, 0.23169325070236535, 0.09844462886708331, 0.046668006223626435, 0.024847756190152438, 0.021680022719973588, 0.030105419228693434, 0.010622110045469348, 0.00901717669102272]\n",
        "MRPC Train Loss:  [0.5656647536100126, 0.3183849928717987, 0.1300674675069019, 0.04107136150602909, 0.018879162419276934, 0.010770226248959992, 0.00942070432367972, 0.011621894868428581, 0.006000024008601173, 0.009634169635782931]\n",
        "RTE Dev Loss:  [0.6476240570728595, 0.8280328443417182, 0.9370238952911817, 1.2024342394792116, 1.3517400049246275, 1.543971258860368, 1.5374018160196452, 1.4505482293092287, 1.8257441291442285, 1.78989472756019]\n",
        "MRPC Dev Loss:  [0.5028427289082453, 0.433395161651648, 0.5221253736661031, 0.7158970440236422, 0.7382933871390728, 0.8112465899724227, 0.8996880593208166, 0.8106272260730083, 1.032469561466804, 0.8509433590448819]\n",
        "RTE Train Acc:  [0.5842524509803921, 0.7913602941176471, 0.9126838235294118, 0.9678308823529411, 0.9846813725490197, 0.9926470588235294, 0.9957107843137255, 0.9905024509803921, 0.9972426470588235, 0.9975490196078431]\n",
        "MRPC Train Acc:  [0.7129289215686274, 0.8759191176470589, 0.9574142156862745, 0.9901960784313726, 0.9954044117647058, 0.9972426470588235, 0.9972426470588235, 0.9966299019607843, 0.9981617647058824, 0.9981617647058824]\n",
        "RTE Dev Acc:  [0.6274038461538461, 0.6887019230769231, 0.6899038461538461, 0.703125, 0.6814903846153846, 0.7043269230769231, 0.6802884615384616, 0.6706730769230769, 0.6742788461538461, 0.6658653846153846]\n",
        "MRPC Dev Acc:  [0.7908653846153846, 0.8149038461538461, 0.8173076923076923, 0.8149038461538461, 0.8365384615384616, 0.8317307692307693, 0.8245192307692307, 0.8185096153846154, 0.8197115384615384, 0.8317307692307693]\n",
        "\n",
        "\n",
        "Task specific vib\n",
        "\n",
        "beta 1e-06 epochs 10\n",
        "Total_train_loss :  [1.2973699055465997, 0.9642873996613073, 0.5546039399855277, 0.28396761694959566, 0.13409733633492507, 0.09744149105915544, 0.08089219421312652, 0.05394729043758821, 0.03974463180730156, 0.034768974413072654]\n",
        "RTE Train Loss:  [0.6997284433420967, 0.5466511889415628, 0.3492789810852093, 0.18960116569902383, 0.09683328131031171, 0.06999995813285019, 0.04584888745026261, 0.029939063589599933, 0.018572569222134704, 0.015015869035029454]\n",
        "MRPC Train Loss:  [0.5735235836576013, 0.3658500596442643, 0.15852692527879103, 0.08246820798947238, 0.03823327874837845, 0.025416642301898523, 0.023801622738806056, 0.02036743158675438, 0.016717976699902805, 0.015579604775434835]\n",
        "RTE Dev Loss:  [0.6728793772367331, 0.7001890803758914, 1.0271583635073442, 1.1211490745727832, 1.3507029620500712, 1.3678690768205202, 1.5518450393126562, 1.6068226465812097, 1.7990044034444368, 1.8880859498794262]\n",
        "MRPC Dev Loss:  [0.5198197880616555, 0.37036060713804686, 0.45410641397421175, 0.5298676634064088, 0.6767484654600804, 0.7056128858373716, 0.7671884940220759, 0.7714589974628046, 0.889338840658848, 0.8442246237626443]\n",
        "RTE Train Acc:  [0.5435049019607843, 0.7270220588235294, 0.8612132352941176, 0.9381127450980392, 0.9718137254901961, 0.9819240196078431, 0.9892769607843137, 0.9920343137254902, 0.9957107843137255, 0.9954044117647058]\n",
        "MRPC Train Acc:  [0.7089460784313726, 0.8480392156862745, 0.9466911764705882, 0.9767156862745098, 0.9901960784313726, 0.9941789215686274, 0.9944852941176471, 0.9960171568627451, 0.9963235294117647, 0.9954044117647058]\n",
        "RTE Dev Acc:  [0.6117788461538461, 0.6658653846153846, 0.6418269230769231, 0.6742788461538461, 0.6706730769230769, 0.6622596153846154, 0.6598557692307693, 0.6790865384615384, 0.6730769230769231, 0.6430288461538461]\n",
        "MRPC Dev Acc:  [0.78125, 0.8413461538461539, 0.828125, 0.8401442307692307, 0.8425480769230769, 0.84375, 0.8329326923076923, 0.8317307692307693, 0.8305288461538461, 0.8341346153846154]\n",
        "\n",
        "Epoch 1 RTE Test Loss: 1.8202778851544414 MRPC Test Loss: 0.8253526733471797\n",
        " RTE Test Acc: 0.6192129629629629 MRPC Test Acc: 0.8365384615384616\n",
        " RTE Test F1: 0.579053996522262 MRPC Test F1: 0.8808820973976637\n",
        "\n",
        "Epoch 1 RTE Test Loss: 0.0 MRPC Test Loss: 0.8798069247493038\n",
        " RTE Test Acc: 0.0 MRPC Test Acc: 0.8232559067688378\n",
        " RTE Test F1: 0.0 MRPC Test F1: 0.8683627113994686\n",
        "\n",
        "beta 1e-03 epochs 10\n",
        "Total_train_loss :  [3.0045245523546256, 1.4005959314458511, 1.3787125687973172, 1.3625180452477699, 1.1669020863140331, 0.9065937978379867, 0.6497531521554086, 0.4059390977901571, 0.3075454977213168, 0.2441467860457944]\n",
        "RTE Train Loss:  [1.3431847931123246, 0.726389093726289, 0.7177418242482578, 0.7168401357005624, 0.6569361353621763, 0.5517127905406204, 0.4004809380161996, 0.25666901885586624, 0.18956677812863798, 0.14279579166688172]\n",
        "MRPC Train Loss:  [1.6117553973899168, 0.6716430479404973, 0.6612728378351997, 0.6356070725356832, 0.4653278763095538, 0.31811319576466784, 0.2095205978143449, 0.13496860040023045, 0.10991633679790824, 0.08878267837651804]\n",
        "RTE Dev Loss:  [0.7385169244729556, 0.7168304576323583, 0.7162205026699946, 0.7026649621816782, 0.6893574755925399, 0.7545248178335336, 0.8414640426635742, 0.9885125595789689, 0.9929910577260531, 1.1224592969967768]\n",
        "MRPC Dev Loss:  [0.6904316246509552, 0.6429093434260442, 0.6406094798674951, 0.588684000647985, 0.5172430414419907, 0.4928063784654324, 0.5115729845487155, 0.5889623279754932, 0.620725291279646, 0.6656962082936213]\n",
        "RTE Train Acc:  [0.49325980392156865, 0.5, 0.49846813725490197, 0.5098039215686274, 0.6528799019607843, 0.7613357843137255, 0.8504901960784313, 0.9319852941176471, 0.9598651960784313, 0.9745710784313726]\n",
        "MRPC Train Acc:  [0.6617647058823529, 0.6706495098039216, 0.6712622549019608, 0.6773897058823529, 0.8170955882352942, 0.9001225490196079, 0.9476102941176471, 0.9742647058823529, 0.9837622549019608, 0.9898897058823529]\n",
        "RTE Dev Acc:  [0.47475961538461536, 0.47716346153846156, 0.47836538461538464, 0.5685096153846154, 0.6394230769230769, 0.609375, 0.6442307692307693, 0.6177884615384616, 0.6334134615384616, 0.6442307692307693]\n",
        "MRPC Dev Acc:  [0.6887019230769231, 0.6959134615384616, 0.6923076923076923, 0.7067307692307693, 0.7824519230769231, 0.7896634615384616, 0.8088942307692307, 0.8040865384615384, 0.8088942307692307, 0.7980769230769231]\n",
        "\n",
        "Epoch 1 RTE Test Loss: 0.0 MRPC Test Loss: 0.6722689911171242\n",
        " RTE Test Acc: 0.0 MRPC Test Acc: 0.7932231800766283\n",
        " RTE Test F1: 0.0 MRPC Test F1: 0.8442451477501932\n",
        "\n",
        "Epoch 1 RTE Test Loss: 1.0826523259834007 MRPC Test Loss: 0.6554780396131369\n",
        " RTE Test Acc: 0.6180555555555556 MRPC Test Acc: 0.7980769230769231\n",
        " RTE Test F1: 0.5973324802743155 MRPC Test F1: 0.8538257842213218\n",
        "\n",
        "beta 1e-02 epochs 10\n",
        "Total_train_loss :  [12.248477047564936, 1.4424947301546733, 1.414850425486471, 1.4095594532349531, 1.4041004940575244]\n",
        "RTE Train Loss:  [7.130209877210505, 0.7507769856967178, 0.7365121595999774, 0.7347381681788201, 0.731864099409066]\n",
        "MRPC Train Loss:  [4.927126979126649, 0.6862177796223584, 0.6802748850747651, 0.6726573214811438, 0.6706259805782169]\n",
        "RTE Dev Loss:  [0.796762975362631, 0.7281370369287637, 0.730153134235969, 0.7275921564835769, 0.7204069449351385]\n",
        "MRPC Dev Loss:  [0.6938107013702393, 0.6498314073452582, 0.6447891123019732, 0.6474772875125592, 0.6388403681608347]\n",
        "RTE Train Acc:  [0.49295343137254904, 0.49816176470588236, 0.5125612745098039, 0.5009191176470589, 0.4987745098039216]\n",
        "MRPC Train Acc:  [0.6541053921568627, 0.6703431372549019, 0.6688112745098039, 0.6697303921568627, 0.6703431372549019]\n",
        "RTE Dev Acc:  [0.5216346153846154, 0.5, 0.49759615384615385, 0.47115384615384615, 0.49759615384615385]\n",
        "MRPC Dev Acc:  [0.6935096153846154, 0.6959134615384616, 0.6923076923076923, 0.6923076923076923, 0.6947115384615384]\n",
        "\n",
        "beta 1e-04 epochs 5\n",
        "Epoch 1 RTE Test Loss: 1.2112525546992268 MRPC Test Loss: 0.615862872881385\n",
        " RTE Test Acc: 0.6319444444444444 MRPC Test Acc: 0.8353365384615384\n",
        " RTE Test F1: 0.543354377831561 MRPC Test F1: 0.8833373624766494\n",
        "\n",
        "Epoch 1 RTE Test Loss: 0.0 MRPC Test Loss: 0.7089270411266221\n",
        " RTE Test Acc: 0.0 MRPC Test Acc: 0.8156130268199234\n",
        " RTE Test F1: 0.0 MRPC Test F1: 0.8665413918334518\n",
        " \n",
        "Total_train_loss :  [1.5298124060911291, 1.1318814403870527, 0.7100894150196337, 0.38627169237417336, 0.20499352909916757]\n",
        "RTE Train Loss:  [0.8119186460971832, 0.6202595131654366, 0.4301571302554187, 0.25138294075926143, 0.13663964386225916]\n",
        "MRPC Train Loss:  [0.7121289617875043, 0.45825673493684504, 0.2316371424525392, 0.10493387037194242, 0.059068543014719206]\n",
        "RTE Dev Loss:  [0.7302864996286539, 0.7429606948907559, 0.8315194570101224, 0.9736692286454715, 1.248211920261383]\n",
        "MRPC Dev Loss:  [0.6006006254599645, 0.4250323480138412, 0.3832165083059898, 0.5369445039675786, 0.6137137103539246]\n",
        "RTE Train Acc:  [0.48621323529411764, 0.6856617647058824, 0.8272058823529411, 0.9172794117647058, 0.9607843137254902]\n",
        "MRPC Train Acc:  [0.6825980392156863, 0.8146446078431373, 0.9338235294117647, 0.9761029411764706, 0.9874387254901961]\n",
        "RTE Dev Acc:  [0.5240384615384616, 0.6370192307692307, 0.6538461538461539, 0.6610576923076923, 0.65625]\n",
        "MRPC Dev Acc:  [0.75, 0.8269230769230769, 0.8509615384615384, 0.8257211538461539, 0.8365384615384616]"
      ],
      "metadata": {
        "id": "ykZRgBMNAfd-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        },
        "outputId": "7a08365d-42ca-47bf-8136-24c97643cd4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-40-9cc0d2d64968>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    beta 1e-03, epochs 10\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "rte_train_loss = [1.1048656228710623, 0.6840196859602835, 0.6065010638797984, 0.49441171715072557, 0.3303750535728885, 0.22947271102491548, 0.17925063075095998, 0.15636680596599392, 0.14810886781881838, 0.11634161586270612]\n",
        "mrpc_train_loss = [1.033039130416571, 0.5938807597347334, 0.506886244988909, 0.39245667337786916, 0.2799694681284474, 0.2109561374082285, 0.16830027979962967, 0.1404654908121801, 0.12685256575544676, 0.10973908222627406]\n",
        "rte_dev_loss  = [0.7373840900567862, 0.8249692297898806, 0.7162189483642578, 0.8412233247206762, 0.9673777130933908, 1.0642799161947691, 1.274667888879776, 1.1392017465371351, 1.3254191394035633, 1.4177790398781116]\n",
        "mrpc_dev_loss = [0.6544087781355932, 0.6297953587311965, 0.5568436017403235, 0.582781576193296, 0.6071517616510391, 0.6568600970965165, 0.7656649901316717, 0.7258225553310834, 0.7412626227507224, 0.7365222435731155]\n",
        "rte_train_loss_1 = [0.7750658556526783, 0.5839595137273564, 0.3623182758981106, 0.18668376771258374, 0.11544408676598002, 0.08070473920773058, 0.058435393168645745, 0.06003689840801206, 0.040856739999178576, 0.041136031075581614]\n",
        "mrpc_train_loss_1 = [0.6888802857959971, 0.4125157302501155, 0.18832684074546777, 0.08987735615422328, 0.0581227570370424, 0.048829952017495445, 0.03593965754935555, 0.03365324404748047, 0.02688894014549898, 0.021642265101785168]\n",
        "rte_dev_loss_1  = [0.6855802169212928, 0.7480734082368704, 0.8674726807154142, 0.9842896335400068, 1.1043801170129042, 1.140833162344419, 1.3175011506447425, 1.3092539929426634, 1.402744316137754, 1.3269579250078936]\n",
        "mrpc_dev_loss_1 = [0.5314731712524707, 0.3557314259501604, 0.4846974473733168, 0.5817678479047922, 0.5850505719964321, 0.5742644340946124, 0.7346780655475763, 0.72365936006491, 0.7643713681743696, 0.6945428596093104]\n",
        "plt.plot(range(len(rte_train_loss)),rte_train_loss,label=\"RTE Train Set, beta 1e-03\")\n",
        "plt.plot(range(len(rte_dev_loss)),rte_dev_loss,label=\"RTE Dev Set, beta 1e-03\")\n",
        "plt.plot(range(len(rte_train_loss_1)),rte_train_loss_1,label=\"RTE Train Set, beta 1e-04\")\n",
        "plt.plot(range(len(rte_dev_loss_1)),rte_dev_loss_1,label=\"RTE Dev Set, beta 1e-04\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(range(len(mrpc_train_loss)),mrpc_train_loss,label=\"MRPC Train Set, beta 1e-03\")\n",
        "plt.plot(range(len(mrpc_dev_loss)),mrpc_dev_loss,label=\"MRPC Dev Set, beta 1e-03\")\n",
        "plt.plot(range(len(mrpc_train_loss_1)),mrpc_train_loss_1,label=\"MRPC Train Set, beta 1e-04\")\n",
        "plt.plot(range(len(mrpc_dev_loss_1)),mrpc_dev_loss_1,label=\"MRPC Dev Set, beta 1e-04\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "id": "MkDwVJ5IqcoE",
        "outputId": "a48ada13-9dd2-4858-d4fb-aa7f8d437f76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score,f1_score\n",
        "\n",
        "n_epochs = 5 #10\n",
        "device=\"cuda\"\n",
        "#vibert.train()\n",
        "vibert.to(device)\n",
        "vibert.zero_grad()\n",
        "set_seed(2022)\n",
        "for epoch in range(n_epochs):\n",
        "  total_train_loss_rte = 0\n",
        "  total_train_loss_mrpc = 0\n",
        "  total_dev_loss_rte = 0\n",
        "  total_dev_loss_mrpc = 0\n",
        "  total_train_acc_rte = 0\n",
        "  total_train_acc_mrpc = 0\n",
        "  total_dev_acc_rte = 0\n",
        "  total_dev_acc_mrpc = 0\n",
        "  vibert.train()\n",
        "  c1_train=0\n",
        "  c2_train=0\n",
        "  for labels,inp1,inp2,st in tqdm(train_dataloader):\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    batch = tokenizer(text=inp1,text_pair=inp2,max_length=128,truncation=True,padding=True,add_special_tokens=True,is_split_into_words=False,return_tensors='pt')\n",
        "    batch.to(device)\n",
        "  \n",
        "    out = vibert(batch[\"input_ids\"],token_type_ids=batch[\"token_type_ids\"], \n",
        "                             attention_mask=batch[\"attention_mask\"],labels=labels.to(device),dataset_name=st[0])\n",
        "    loss = out[\"loss\"][\"loss\"]\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()  # Update learning rate schedule\n",
        "    vibert.zero_grad()\n",
        "    \n",
        "    if st[0]==\"rte\":\n",
        "      total_train_loss_rte += loss.item()\n",
        "      total_train_acc_rte += accuracy_score(labels.cpu(),torch.argmax(out[\"logits\"],1).cpu())  \n",
        "      c1_train+=1\n",
        "    if st[0]==\"mrpc\":\n",
        "      total_train_loss_mrpc += loss.item()\n",
        "      total_train_acc_mrpc += accuracy_score(labels.cpu(),torch.argmax(out[\"logits\"],1).cpu())\n",
        "      c2_train+=1\n",
        "    torch.nn.utils.clip_grad_norm_(vibert.parameters(), 1.0)\n",
        "  \n",
        "  print(\"Epoch \"+str(epoch)+\" RTE Train Loss: \"+str(total_train_loss_rte/c1_train)+\" MRPC Train Loss: \"+str(total_train_loss_mrpc/c2_train))\n",
        "  print(\" RTE Train Acc: \"+str(total_train_acc_rte/c1_train)+\" MRPC Train Acc: \"+str(total_train_acc_mrpc/c2_train))\n",
        "  \n",
        "  vibert.eval()\n",
        "  c1_dev=0\n",
        "  c2_dev=0\n",
        "  for labels,inp1,inp2,st in tqdm(dev_dataloader):\n",
        "    \n",
        "    batch = tokenizer(text=inp1,text_pair=inp2,max_length=128,truncation=True,padding=True,is_split_into_words=False,return_tensors='pt')\n",
        "    batch.to(device)\n",
        "    \n",
        "    out = vibert(batch[\"input_ids\"],token_type_ids=batch[\"token_type_ids\"], \n",
        "                             attention_mask=batch[\"attention_mask\"],labels=labels.to(device),dataset_name=st[0])\n",
        "    loss = out[\"loss\"][\"loss\"]\n",
        "    if st[0]==\"rte\":\n",
        "      total_dev_loss_rte += loss.item()\n",
        "      total_dev_acc_rte += accuracy_score(labels.cpu(),torch.argmax(out[\"logits\"],1).cpu())\n",
        "      c1_dev+=1\n",
        "    if st[0]==\"mrpc\":\n",
        "      total_dev_loss_mrpc += loss.item()\n",
        "      total_dev_acc_mrpc += accuracy_score(labels.cpu(),torch.argmax(out[\"logits\"],1).cpu())\n",
        "      c2_dev+=1\n",
        "     \n",
        "  print(\"Epoch \"+str(epoch)+\" RTE Val Loss: \"+str(total_dev_loss_rte/c1_dev)+\" MRPC Val Loss: \"+str(total_dev_loss_mrpc/c2_dev))\n",
        "  print(\" RTE Val Acc: \"+str(total_dev_acc_rte/c1_dev)+\" MRPC Val Acc: \"+str(total_dev_acc_mrpc/c2_dev))\n",
        "  \n",
        "  torch.save(vibert.state_dict(), \"vibert_70_new\")"
      ],
      "metadata": {
        "id": "E9UTT7OKQVPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "device=\"cuda\"\n",
        "#vibert.train()\n",
        "vibert.to(device)\n",
        "set_seed(2022)\n",
        "vibert.eval()\n",
        "total_test_loss_rte = 0\n",
        "total_test_loss_mrpc = 0\n",
        "total_test_acc_rte = 0\n",
        "total_test_acc_mrpc = 0\n",
        "total_test_f1_rte = 0\n",
        "total_test_f1_mrpc = 0\n",
        "c1=1\n",
        "c2=0\n",
        "epoch=1\n",
        "for labels,inp1,inp2,st in tqdm(mrpc_test_dataloader):\n",
        "  \n",
        "  batch = tokenizer(text=inp1,text_pair=inp2,max_length=128,truncation=True,padding=True,is_split_into_words=False,return_tensors='pt')\n",
        "  batch.to(device)\n",
        "  out = vibert(batch[\"input_ids\"],token_type_ids=batch[\"token_type_ids\"], \n",
        "                             attention_mask=batch[\"attention_mask\"],labels=labels.to(device),dataset_name=st[0],logits=None,label=None,sampled_logits=None,kl_loss=None)\n",
        "  loss = out[\"loss\"][\"loss\"]\n",
        "  if st[0]==\"rte\":\n",
        "    total_test_loss_rte += loss.item()\n",
        "    total_test_acc_rte += accuracy_score(labels.cpu(),torch.argmax(out[\"logits\"],1).cpu())\n",
        "    total_test_f1_rte += f1_score(labels.cpu(),torch.argmax(out[\"logits\"],1).cpu())\n",
        "    c1+=1\n",
        "  if st[0]==\"mrpc\":\n",
        "    total_test_loss_mrpc += loss.item()\n",
        "    total_test_acc_mrpc += accuracy_score(labels.cpu(),torch.argmax(out[\"logits\"],1).cpu())\n",
        "    total_test_f1_mrpc += f1_score(labels.cpu(),torch.argmax(out[\"logits\"],1).cpu())\n",
        "    c2+=1\n",
        "    \n",
        "print(\"Epoch \"+str(epoch)+\" RTE Test Loss: \"+str(total_test_loss_rte/c1)+\" MRPC Test Loss: \"+str(total_test_loss_mrpc/c2))\n",
        "print(\" RTE Test Acc: \"+str(total_test_acc_rte/c1)+\" MRPC Test Acc: \"+str(total_test_acc_mrpc/c2))\n",
        "print(\" RTE Test F1: \"+str(total_test_f1_rte/c1)+\" MRPC Test F1: \"+str(total_test_f1_mrpc/c2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qj5hVLOAeR2",
        "outputId": "757aaf8d-b707-4861-9b8d-17796acdca7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 54/54 [00:11<00:00,  4.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 RTE Test Loss: 0.0 MRPC Test Loss: 0.7089270411266221\n",
            " RTE Test Acc: 0.0 MRPC Test Acc: 0.8156130268199234\n",
            " RTE Test F1: 0.0 MRPC Test F1: 0.8665413918334518\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "device=\"cuda\"\n",
        "#vibert.train()\n",
        "vibert.to(device)\n",
        "set_seed(2022)\n",
        "vibert.eval()\n",
        "total_test_loss_rte = 0\n",
        "total_test_loss_mrpc = 0\n",
        "total_test_acc_rte = 0\n",
        "total_test_acc_mrpc = 0\n",
        "total_test_f1_rte = 0\n",
        "total_test_f1_mrpc = 0\n",
        "c1=1\n",
        "c2=0\n",
        "epoch=1\n",
        "for labels,inp1,inp2,st in tqdm(dev_dataloader):\n",
        "  \n",
        "  batch = tokenizer(text=inp1,text_pair=inp2,max_length=128,truncation=True,padding=True,is_split_into_words=False,return_tensors='pt')\n",
        "  batch.to(device)\n",
        "  out = vibert(batch[\"input_ids\"],token_type_ids=batch[\"token_type_ids\"], \n",
        "                             attention_mask=batch[\"attention_mask\"],labels=labels.to(device),dataset_name=st[0],logits=None,label=None,sampled_logits=None,kl_loss=None)\n",
        "  loss = out[\"loss\"][\"loss\"]\n",
        "  if st[0]==\"rte\":\n",
        "    total_test_loss_rte += loss.item()\n",
        "    total_test_acc_rte += accuracy_score(labels.cpu(),torch.argmax(out[\"logits\"],1).cpu())\n",
        "    total_test_f1_rte += f1_score(labels.cpu(),torch.argmax(out[\"logits\"],1).cpu())\n",
        "    c1+=1\n",
        "  if st[0]==\"mrpc\":\n",
        "    total_test_loss_mrpc += loss.item()\n",
        "    total_test_acc_mrpc += accuracy_score(labels.cpu(),torch.argmax(out[\"logits\"],1).cpu())\n",
        "    total_test_f1_mrpc += f1_score(labels.cpu(),torch.argmax(out[\"logits\"],1).cpu())\n",
        "    c2+=1\n",
        "    \n",
        "print(\"Epoch \"+str(epoch)+\" RTE Test Loss: \"+str(total_test_loss_rte/c1)+\" MRPC Test Loss: \"+str(total_test_loss_mrpc/c2))\n",
        "print(\" RTE Test Acc: \"+str(total_test_acc_rte/c1)+\" MRPC Test Acc: \"+str(total_test_acc_mrpc/c2))\n",
        "print(\" RTE Test F1: \"+str(total_test_f1_rte/c1)+\" MRPC Test F1: \"+str(total_test_f1_mrpc/c2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ka26VP2M0hKl",
        "outputId": "a442ef14-3392-4234-fd1e-a925f6e41f95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:12<00:00,  4.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 RTE Test Loss: 1.2112525546992268 MRPC Test Loss: 0.615862872881385\n",
            " RTE Test Acc: 0.6319444444444444 MRPC Test Acc: 0.8353365384615384\n",
            " RTE Test F1: 0.543354377831561 MRPC Test F1: 0.8833373624766494\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "device=\"cuda\"\n",
        "#vibert.train()\n",
        "vibert.to(device)\n",
        "set_seed(2022)\n",
        "vibert.eval()\n",
        "total_test_loss_rte = 0\n",
        "total_test_loss_mrpc = 0\n",
        "total_test_acc_rte = 0\n",
        "total_test_acc_mrpc = 0\n",
        "total_test_f1_rte = 0\n",
        "total_test_f1_mrpc = 0\n",
        "c1=1\n",
        "c2=0\n",
        "epoch=1\n",
        "for labels,inp1,inp2,st in tqdm(dev_dataloader):\n",
        "  \n",
        "  batch = tokenizer(text=inp1,text_pair=inp2,max_length=128,truncation=True,padding=True,is_split_into_words=False,return_tensors='pt')\n",
        "  batch.to(device)\n",
        "  out = vibert(batch[\"input_ids\"],token_type_ids=batch[\"token_type_ids\"], \n",
        "                             attention_mask=batch[\"attention_mask\"],labels=labels.to(device),dataset_name=st[0],logits=None,label=None,sampled_logits=None,kl_loss=None)\n",
        "  loss = out[\"loss\"][\"loss\"]\n",
        "  if st[0]==\"rte\":\n",
        "    total_test_loss_rte += loss.item()\n",
        "    total_test_acc_rte += accuracy_score(labels.cpu(),torch.argmax(out[\"logits\"],1).cpu())\n",
        "    total_test_f1_rte += f1_score(labels.cpu(),torch.argmax(out[\"logits\"],1).cpu())\n",
        "    c1+=1\n",
        "  if st[0]==\"mrpc\":\n",
        "    total_test_loss_mrpc += loss.item()\n",
        "    total_test_acc_mrpc += accuracy_score(labels.cpu(),torch.argmax(out[\"logits\"],1).cpu())\n",
        "    total_test_f1_mrpc += f1_score(labels.cpu(),torch.argmax(out[\"logits\"],1).cpu())\n",
        "    c2+=1\n",
        "    \n",
        "print(\"Epoch \"+str(epoch)+\" RTE Test Loss: \"+str(total_test_loss_rte/c1)+\" MRPC Test Loss: \"+str(total_test_loss_mrpc/c2))\n",
        "print(\" RTE Test Acc: \"+str(total_test_acc_rte/c1)+\" MRPC Test Acc: \"+str(total_test_acc_mrpc/c2))\n",
        "print(\" RTE Test F1: \"+str(total_test_f1_rte/c1)+\" MRPC Test F1: \"+str(total_test_f1_mrpc/c2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbsMLwEy0kc9",
        "outputId": "45d548c2-9a29-4a45-84aa-eb38650c61d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:10<00:00,  4.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 RTE Test Loss: 1.280003062000981 MRPC Test Loss: 0.6871244019040694\n",
            " RTE Test Acc: 0.6527777777777778 MRPC Test Acc: 0.828125\n",
            " RTE Test F1: 0.6436523232285588 MRPC Test F1: 0.8765453009343809\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "device=\"cuda\"\n",
        "#vibert.train()\n",
        "vibert.to(device)\n",
        "set_seed(2022)\n",
        "vibert.eval()\n",
        "total_test_loss_rte = 0\n",
        "total_test_loss_mrpc = 0\n",
        "total_test_acc_rte = 0\n",
        "total_test_acc_mrpc = 0\n",
        "total_test_f1_rte = 0\n",
        "total_test_f1_mrpc = 0\n",
        "c1=1\n",
        "c2=0\n",
        "epoch=1\n",
        "for labels,inp1,inp2,st in tqdm(dev_dataloader):\n",
        "  \n",
        "  batch = tokenizer(text=inp1,text_pair=inp2,max_length=128,truncation=True,padding=True,is_split_into_words=False,return_tensors='pt')\n",
        "  batch.to(device)\n",
        "  out = vibert(batch[\"input_ids\"],token_type_ids=batch[\"token_type_ids\"], \n",
        "                             attention_mask=batch[\"attention_mask\"],labels=labels.to(device),dataset_name=st[0],logits=None,label=None,sampled_logits=None,kl_loss=None)\n",
        "  loss = out[\"loss\"][\"loss\"]\n",
        "  if st[0]==\"rte\":\n",
        "    total_test_loss_rte += loss.item()\n",
        "    total_test_acc_rte += accuracy_score(labels.cpu(),torch.argmax(out[\"logits\"],1).cpu())\n",
        "    total_test_f1_rte += f1_score(labels.cpu(),torch.argmax(out[\"logits\"],1).cpu())\n",
        "    c1+=1\n",
        "  if st[0]==\"mrpc\":\n",
        "    total_test_loss_mrpc += loss.item()\n",
        "    total_test_acc_mrpc += accuracy_score(labels.cpu(),torch.argmax(out[\"logits\"],1).cpu())\n",
        "    total_test_f1_mrpc += f1_score(labels.cpu(),torch.argmax(out[\"logits\"],1).cpu())\n",
        "    c2+=1\n",
        "    \n",
        "print(\"Epoch \"+str(epoch)+\" RTE Test Loss: \"+str(total_test_loss_rte/c1)+\" MRPC Test Loss: \"+str(total_test_loss_mrpc/c2))\n",
        "print(\" RTE Test Acc: \"+str(total_test_acc_rte/c1)+\" MRPC Test Acc: \"+str(total_test_acc_mrpc/c2))\n",
        "print(\" RTE Test F1: \"+str(total_test_f1_rte/c1)+\" MRPC Test F1: \"+str(total_test_f1_mrpc/c2))"
      ],
      "metadata": {
        "id": "E97S28K0inIE",
        "outputId": "8a115592-eaa5-43d4-8a8b-fc12dcc65573",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:12<00:00,  4.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 RTE Test Loss: 1.4761454926596747 MRPC Test Loss: 0.7835625788340201\n",
            " RTE Test Acc: 0.5891203703703703 MRPC Test Acc: 0.7776442307692307\n",
            " RTE Test F1: 0.5842723926370899 MRPC Test F1: 0.836523252732936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import CrossEntropyLoss, MSELoss\n",
        "from transformers import RobertaPreTrainedModel, RobertaModel,RobertaConfig\n",
        "class RobertaForSequenceClassification(RobertaPreTrainedModel):\n",
        "    def __init__(self,config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = 2\n",
        "        #config = BertConfig()\n",
        "        self.roberta = RobertaModel(config)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.deterministic = False\n",
        "        self.ib_dim = 384\n",
        "        self.ib = True\n",
        "        #self.deterministic = True\n",
        "        self.activation = 'relu'\n",
        "        self.activations = {'tanh': nn.Tanh(), 'relu': nn.ReLU(), 'sigmoid': nn.Sigmoid()}\n",
        "        if self.ib or self.deterministic:\n",
        "            self.kl_annealing = \"linear\"\n",
        "            self.hidden_dim = (768 + self.ib_dim) // 2\n",
        "            intermediate_dim = (self.hidden_dim+768)//2\n",
        "            self.mlp_rte = nn.Sequential(\n",
        "                nn.Linear(768, intermediate_dim), #768\n",
        "                self.activations[self.activation],\n",
        "                nn.Linear(intermediate_dim, self.hidden_dim),\n",
        "                self.activations[self.activation])\n",
        "            self.mlp_mrpc = nn.Sequential(\n",
        "                nn.Linear(768, intermediate_dim), #768\n",
        "                self.activations[self.activation],\n",
        "                nn.Linear(intermediate_dim, self.hidden_dim),\n",
        "                self.activations[self.activation])\n",
        "            self.beta = 1e-04\n",
        "            self.sample_size = 5 \n",
        "            self.emb2mu_rte = nn.Linear(self.hidden_dim, self.ib_dim)\n",
        "            self.emb2std_rte = nn.Linear(self.hidden_dim, self.ib_dim)\n",
        "            self.mu_p_rte = nn.Parameter(torch.randn(self.ib_dim))\n",
        "            self.std_p_rte = nn.Parameter(torch.randn(self.ib_dim))\n",
        "            self.emb2mu_mrpc = nn.Linear(self.hidden_dim, self.ib_dim)\n",
        "            self.emb2std_mrpc = nn.Linear(self.hidden_dim, self.ib_dim)\n",
        "            self.mu_p_mrpc = nn.Parameter(torch.randn(self.ib_dim))\n",
        "            self.std_p_mrpc = nn.Parameter(torch.randn(self.ib_dim))\n",
        "            self.classifier_rte = nn.Linear(self.ib_dim, self.num_labels)\n",
        "            self.classifier_mrpc = nn.Linear(self.ib_dim, self.num_labels) \n",
        "        else:\n",
        "            self.classifier = nn.Linear(768, self.num_labels)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def estimate(self, emb, emb2mu, emb2std):\n",
        "        \"\"\"Estimates mu and std from the given input embeddings.\"\"\"\n",
        "        mean = emb2mu(emb)\n",
        "        std = torch.nn.functional.softplus(emb2std(emb))\n",
        "        return mean, std\n",
        "\n",
        "    def kl_div(self, mu_q, std_q, mu_p, std_p):\n",
        "        \"\"\"Computes the KL divergence between the two given variational distribution.\\\n",
        "           This computes KL(q||p), which is not symmetric. It quantifies how far is\\\n",
        "           The estimated distribution q from the true distribution of p.\"\"\"\n",
        "        k = mu_q.size(1)\n",
        "        mu_diff = mu_p - mu_q\n",
        "        mu_diff_sq = torch.mul(mu_diff, mu_diff)\n",
        "        logdet_std_q = torch.sum(2 * torch.log(torch.clamp(std_q, min=1e-8)), dim=1)\n",
        "        logdet_std_p = torch.sum(2 * torch.log(torch.clamp(std_p, min=1e-8)), dim=1)\n",
        "        fs = torch.sum(torch.div(std_q ** 2, std_p ** 2), dim=1) + torch.sum(torch.div(mu_diff_sq, std_p ** 2), dim=1)\n",
        "        kl_divergence = (fs - k + logdet_std_p - logdet_std_q)*0.5\n",
        "        return kl_divergence.mean()\n",
        "\n",
        "    def reparameterize(self, mu, std):\n",
        "        batch_size = mu.shape[0]\n",
        "        z = torch.randn(self.sample_size, batch_size, mu.shape[1]).cuda()\n",
        "        return mu + std * z\n",
        "\n",
        "    def get_logits(self, z, mu, sampling_type,dataset_name):\n",
        "        if sampling_type == \"iid\":\n",
        "            if dataset_name == \"rte\":\n",
        "              logits = self.classifier_rte(z)\n",
        "            else:\n",
        "              logits = self.classifier_mrpc(z)\n",
        "            mean_logits = logits.mean(dim=0)\n",
        "            logits = logits.permute(1, 2, 0)\n",
        "        else:\n",
        "            if dataset_name == 0:\n",
        "              mean_logits = self.classifier_rte(mu)\n",
        "            else:\n",
        "              mean_logits = self.classifier_mrpc(mu)\n",
        "            #mean_logits = self.classifier(mu)\n",
        "            logits = mean_logits\n",
        "        return logits, mean_logits\n",
        "\n",
        "\n",
        "    def sampled_loss(self, logits, mean_logits, labels, sampling_type):\n",
        "        if sampling_type == \"iid\":\n",
        "            # During the training, computes the loss with the sampled embeddings.\n",
        "            if self.num_labels == 1:\n",
        "                #  We are doing regression\n",
        "                loss_fct = MSELoss()\n",
        "                loss = loss_fct(logits.view(-1, self.sample_size), labels[:, None].float().expand(-1, self.sample_size))\n",
        "                loss = torch.mean(loss, dim=-1)\n",
        "                loss = torch.mean(loss, dim=0)\n",
        "            else:\n",
        "                loss_fct = CrossEntropyLoss(reduce=False)\n",
        "                loss = loss_fct(logits, labels[:, None].expand(-1, self.sample_size))\n",
        "                loss = torch.mean(loss, dim=-1)\n",
        "                loss = torch.mean(loss, dim=0)\n",
        "        else:\n",
        "            # During test time, uses the average value for prediction.\n",
        "            if self.num_labels == 1:\n",
        "                loss_fct = MSELoss()\n",
        "                loss = loss_fct(mean_logits.view(-1), labels.float().view(-1))\n",
        "            else:\n",
        "                loss_fct = CrossEntropyLoss()\n",
        "                loss = loss_fct(mean_logits, labels)\n",
        "        return loss\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        sampling_type=\"iid\",\n",
        "        epoch=1,\n",
        "        **kwargs\n",
        "        #dataset_name=\"rte\",\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`, defaults to :obj:`None`):\n",
        "            Labels for computing the sequence classification/regression loss.\n",
        "            Indices should be in :obj:`[0, ..., config.num_labels - 1]`.\n",
        "            If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n",
        "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
        "    Returns:\n",
        "        :obj:`tuple(torch.FloatTensor)` comprising various elements depending on the configuration (:class:`~transformers.BertConfig`) and inputs:\n",
        "        loss (:obj:`torch.FloatTensor` of shape :obj:`(1,)`, `optional`, returned when :obj:`label` is provided):\n",
        "            Classification (or regression if config.num_labels==1) loss.\n",
        "        logits (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, config.num_labels)`):\n",
        "            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
        "        hidden_states (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_hidden_states=True``):\n",
        "            Tuple of :obj:`torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)\n",
        "            of shape :obj:`(batch_size, sequence_length, hidden_size)`.\n",
        "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
        "        attentions (:obj:`tuple(torch.FloatTensor)`, `optional`, returned when ``config.output_attentions=True``):\n",
        "            Tuple of :obj:`torch.FloatTensor` (one for each layer) of shape\n",
        "            :obj:`(batch_size, num_heads, sequence_length, sequence_length)`.\n",
        "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
        "            heads.\n",
        "    Examples::\n",
        "        from transformers import BertTokenizer, BertForSequenceClassification\n",
        "        import torch\n",
        "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
        "        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
        "        outputs = model(input_ids, labels=labels)\n",
        "        loss, logits = outputs[:2]\n",
        "        \"\"\"\n",
        "        #dataset_name=\"rte\"\n",
        "        #print(position_ids.item())\n",
        "        position_id = None\n",
        "        final_outputs = {}\n",
        "        outputs = self.roberta(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_id,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "        )\n",
        "        pooled_output = outputs[1]\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        loss = {}\n",
        "\n",
        "        if self.deterministic:\n",
        "            pooled_output = self.mlp(pooled_output)\n",
        "            mu, std = self.estimate(pooled_output, self.emb2mu, self.emb2std)\n",
        "            final_outputs[\"z\"] = mu\n",
        "            sampled_logits, logits = self.get_logits(mu, mu, sampling_type='argmax',dataset_name=kwargs[\"dataset_name\"]) # always deterministic\n",
        "            if labels is not None:\n",
        "                loss[\"loss\"] = self.sampled_loss(sampled_logits, logits, labels.view(-1), sampling_type='argmax')\n",
        "\n",
        "        elif self.ib:\n",
        "            if kwargs[\"dataset_name\"]==\"rte\":\n",
        "              pooled_output = self.mlp_rte(pooled_output)\n",
        "              batch_size = pooled_output.shape[0]\n",
        "              mu, std = self.estimate(pooled_output, self.emb2mu_rte, self.emb2std_rte)\n",
        "              mu_p = self.mu_p_rte.view(1, -1).expand(batch_size, -1)\n",
        "              std_p = torch.nn.functional.softplus(self.std_p_rte.view(1, -1).expand(batch_size, -1))\n",
        "            else:\n",
        "              pooled_output = self.mlp_mrpc(pooled_output)\n",
        "              batch_size = pooled_output.shape[0]\n",
        "              mu, std = self.estimate(pooled_output, self.emb2mu_mrpc, self.emb2std_mrpc)\n",
        "              mu_p = self.mu_p_mrpc.view(1, -1).expand(batch_size, -1)\n",
        "              std_p = torch.nn.functional.softplus(self.std_p_mrpc.view(1, -1).expand(batch_size, -1))\n",
        "            kl_loss = self.kl_div(mu, std, mu_p, std_p)\n",
        "            z = self.reparameterize(mu, std)\n",
        "            final_outputs[\"z\"] = mu\n",
        "\n",
        "            if self.kl_annealing == \"linear\":\n",
        "                beta = min(1.0, epoch*self.beta)\n",
        "                 \n",
        "            sampled_logits, logits = self.get_logits(z, mu, sampling_type,dataset_name=kwargs[\"dataset_name\"])\n",
        "            #print(labels)\n",
        "            if labels is not None:\n",
        "                if kwargs[\"label\"] is not None:\n",
        "                  ce_loss_rte = self.sampled_loss(kwargs[\"sampled_logits\"], kwargs[\"logits\"], kwargs[\"label\"].view(-1), sampling_type)\n",
        "                  total_loss_rte = ce_loss_rte + (beta if self.kl_annealing == \"linear\" else self.beta) * kwargs[\"kl_loss\"]\n",
        "                  ce_loss_mrpc = self.sampled_loss(sampled_logits, logits, labels.view(-1), sampling_type)\n",
        "                  total_loss_mrpc = ce_loss_mrpc + (beta if self.kl_annealing == \"linear\" else self.beta) * kl_loss\n",
        "                  total_loss = total_loss_rte + total_loss_mrpc\n",
        "                else:\n",
        "                  ce_loss = self.sampled_loss(sampled_logits, logits, labels.view(-1), sampling_type)\n",
        "                  total_loss = ce_loss + (beta if self.kl_annealing == \"linear\" else self.beta) * kl_loss\n",
        "                loss[\"loss\"] = total_loss\n",
        "        else:\n",
        "            final_outputs[\"z\"] = pooled_output\n",
        "            logits = self.classifier(pooled_output)\n",
        "            if labels is not None:\n",
        "                if self.num_labels == 1:\n",
        "                    #  We are doing regression\n",
        "                    loss_fct = MSELoss()\n",
        "                    loss[\"loss\"] = loss_fct(logits.view(-1), labels.float().view(-1))\n",
        "                else:\n",
        "                    loss_fct = CrossEntropyLoss()\n",
        "                    loss[\"loss\"] = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "                    \n",
        "        final_outputs.update({\"logits\": logits, \"loss\": loss, \"hidden_attention\": outputs[2:],\"sampled_logits\":sampled_logits,\"kl_loss\":kl_loss})\n",
        "        return final_outputs"
      ],
      "metadata": {
        "id": "pq3eLMvZdNsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaTokenizer\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base', do_lower_case=True)\n",
        "config = RobertaConfig.from_pretrained(\n",
        "        \"roberta-base\",\n",
        "        num_labels=2)\n",
        "viroberta = RobertaForSequenceClassification.from_pretrained(\n",
        "        \"roberta-base\",\n",
        "        config=config\n",
        ")"
      ],
      "metadata": {
        "id": "ueGC-jOwmgE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in viroberta.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFPQlmWAm-2D",
        "outputId": "bfeaef1c-257d-4446-cf29-ad29ed4d70b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mu_p_rte\n",
            "std_p_rte\n",
            "mu_p_mrpc\n",
            "std_p_mrpc\n",
            "roberta.embeddings.word_embeddings.weight\n",
            "roberta.embeddings.position_embeddings.weight\n",
            "roberta.embeddings.token_type_embeddings.weight\n",
            "roberta.embeddings.LayerNorm.weight\n",
            "roberta.embeddings.LayerNorm.bias\n",
            "roberta.encoder.layer.0.attention.self.query.weight\n",
            "roberta.encoder.layer.0.attention.self.query.bias\n",
            "roberta.encoder.layer.0.attention.self.key.weight\n",
            "roberta.encoder.layer.0.attention.self.key.bias\n",
            "roberta.encoder.layer.0.attention.self.value.weight\n",
            "roberta.encoder.layer.0.attention.self.value.bias\n",
            "roberta.encoder.layer.0.attention.output.dense.weight\n",
            "roberta.encoder.layer.0.attention.output.dense.bias\n",
            "roberta.encoder.layer.0.attention.output.LayerNorm.weight\n",
            "roberta.encoder.layer.0.attention.output.LayerNorm.bias\n",
            "roberta.encoder.layer.0.intermediate.dense.weight\n",
            "roberta.encoder.layer.0.intermediate.dense.bias\n",
            "roberta.encoder.layer.0.output.dense.weight\n",
            "roberta.encoder.layer.0.output.dense.bias\n",
            "roberta.encoder.layer.0.output.LayerNorm.weight\n",
            "roberta.encoder.layer.0.output.LayerNorm.bias\n",
            "roberta.encoder.layer.1.attention.self.query.weight\n",
            "roberta.encoder.layer.1.attention.self.query.bias\n",
            "roberta.encoder.layer.1.attention.self.key.weight\n",
            "roberta.encoder.layer.1.attention.self.key.bias\n",
            "roberta.encoder.layer.1.attention.self.value.weight\n",
            "roberta.encoder.layer.1.attention.self.value.bias\n",
            "roberta.encoder.layer.1.attention.output.dense.weight\n",
            "roberta.encoder.layer.1.attention.output.dense.bias\n",
            "roberta.encoder.layer.1.attention.output.LayerNorm.weight\n",
            "roberta.encoder.layer.1.attention.output.LayerNorm.bias\n",
            "roberta.encoder.layer.1.intermediate.dense.weight\n",
            "roberta.encoder.layer.1.intermediate.dense.bias\n",
            "roberta.encoder.layer.1.output.dense.weight\n",
            "roberta.encoder.layer.1.output.dense.bias\n",
            "roberta.encoder.layer.1.output.LayerNorm.weight\n",
            "roberta.encoder.layer.1.output.LayerNorm.bias\n",
            "roberta.encoder.layer.2.attention.self.query.weight\n",
            "roberta.encoder.layer.2.attention.self.query.bias\n",
            "roberta.encoder.layer.2.attention.self.key.weight\n",
            "roberta.encoder.layer.2.attention.self.key.bias\n",
            "roberta.encoder.layer.2.attention.self.value.weight\n",
            "roberta.encoder.layer.2.attention.self.value.bias\n",
            "roberta.encoder.layer.2.attention.output.dense.weight\n",
            "roberta.encoder.layer.2.attention.output.dense.bias\n",
            "roberta.encoder.layer.2.attention.output.LayerNorm.weight\n",
            "roberta.encoder.layer.2.attention.output.LayerNorm.bias\n",
            "roberta.encoder.layer.2.intermediate.dense.weight\n",
            "roberta.encoder.layer.2.intermediate.dense.bias\n",
            "roberta.encoder.layer.2.output.dense.weight\n",
            "roberta.encoder.layer.2.output.dense.bias\n",
            "roberta.encoder.layer.2.output.LayerNorm.weight\n",
            "roberta.encoder.layer.2.output.LayerNorm.bias\n",
            "roberta.encoder.layer.3.attention.self.query.weight\n",
            "roberta.encoder.layer.3.attention.self.query.bias\n",
            "roberta.encoder.layer.3.attention.self.key.weight\n",
            "roberta.encoder.layer.3.attention.self.key.bias\n",
            "roberta.encoder.layer.3.attention.self.value.weight\n",
            "roberta.encoder.layer.3.attention.self.value.bias\n",
            "roberta.encoder.layer.3.attention.output.dense.weight\n",
            "roberta.encoder.layer.3.attention.output.dense.bias\n",
            "roberta.encoder.layer.3.attention.output.LayerNorm.weight\n",
            "roberta.encoder.layer.3.attention.output.LayerNorm.bias\n",
            "roberta.encoder.layer.3.intermediate.dense.weight\n",
            "roberta.encoder.layer.3.intermediate.dense.bias\n",
            "roberta.encoder.layer.3.output.dense.weight\n",
            "roberta.encoder.layer.3.output.dense.bias\n",
            "roberta.encoder.layer.3.output.LayerNorm.weight\n",
            "roberta.encoder.layer.3.output.LayerNorm.bias\n",
            "roberta.encoder.layer.4.attention.self.query.weight\n",
            "roberta.encoder.layer.4.attention.self.query.bias\n",
            "roberta.encoder.layer.4.attention.self.key.weight\n",
            "roberta.encoder.layer.4.attention.self.key.bias\n",
            "roberta.encoder.layer.4.attention.self.value.weight\n",
            "roberta.encoder.layer.4.attention.self.value.bias\n",
            "roberta.encoder.layer.4.attention.output.dense.weight\n",
            "roberta.encoder.layer.4.attention.output.dense.bias\n",
            "roberta.encoder.layer.4.attention.output.LayerNorm.weight\n",
            "roberta.encoder.layer.4.attention.output.LayerNorm.bias\n",
            "roberta.encoder.layer.4.intermediate.dense.weight\n",
            "roberta.encoder.layer.4.intermediate.dense.bias\n",
            "roberta.encoder.layer.4.output.dense.weight\n",
            "roberta.encoder.layer.4.output.dense.bias\n",
            "roberta.encoder.layer.4.output.LayerNorm.weight\n",
            "roberta.encoder.layer.4.output.LayerNorm.bias\n",
            "roberta.encoder.layer.5.attention.self.query.weight\n",
            "roberta.encoder.layer.5.attention.self.query.bias\n",
            "roberta.encoder.layer.5.attention.self.key.weight\n",
            "roberta.encoder.layer.5.attention.self.key.bias\n",
            "roberta.encoder.layer.5.attention.self.value.weight\n",
            "roberta.encoder.layer.5.attention.self.value.bias\n",
            "roberta.encoder.layer.5.attention.output.dense.weight\n",
            "roberta.encoder.layer.5.attention.output.dense.bias\n",
            "roberta.encoder.layer.5.attention.output.LayerNorm.weight\n",
            "roberta.encoder.layer.5.attention.output.LayerNorm.bias\n",
            "roberta.encoder.layer.5.intermediate.dense.weight\n",
            "roberta.encoder.layer.5.intermediate.dense.bias\n",
            "roberta.encoder.layer.5.output.dense.weight\n",
            "roberta.encoder.layer.5.output.dense.bias\n",
            "roberta.encoder.layer.5.output.LayerNorm.weight\n",
            "roberta.encoder.layer.5.output.LayerNorm.bias\n",
            "roberta.encoder.layer.6.attention.self.query.weight\n",
            "roberta.encoder.layer.6.attention.self.query.bias\n",
            "roberta.encoder.layer.6.attention.self.key.weight\n",
            "roberta.encoder.layer.6.attention.self.key.bias\n",
            "roberta.encoder.layer.6.attention.self.value.weight\n",
            "roberta.encoder.layer.6.attention.self.value.bias\n",
            "roberta.encoder.layer.6.attention.output.dense.weight\n",
            "roberta.encoder.layer.6.attention.output.dense.bias\n",
            "roberta.encoder.layer.6.attention.output.LayerNorm.weight\n",
            "roberta.encoder.layer.6.attention.output.LayerNorm.bias\n",
            "roberta.encoder.layer.6.intermediate.dense.weight\n",
            "roberta.encoder.layer.6.intermediate.dense.bias\n",
            "roberta.encoder.layer.6.output.dense.weight\n",
            "roberta.encoder.layer.6.output.dense.bias\n",
            "roberta.encoder.layer.6.output.LayerNorm.weight\n",
            "roberta.encoder.layer.6.output.LayerNorm.bias\n",
            "roberta.encoder.layer.7.attention.self.query.weight\n",
            "roberta.encoder.layer.7.attention.self.query.bias\n",
            "roberta.encoder.layer.7.attention.self.key.weight\n",
            "roberta.encoder.layer.7.attention.self.key.bias\n",
            "roberta.encoder.layer.7.attention.self.value.weight\n",
            "roberta.encoder.layer.7.attention.self.value.bias\n",
            "roberta.encoder.layer.7.attention.output.dense.weight\n",
            "roberta.encoder.layer.7.attention.output.dense.bias\n",
            "roberta.encoder.layer.7.attention.output.LayerNorm.weight\n",
            "roberta.encoder.layer.7.attention.output.LayerNorm.bias\n",
            "roberta.encoder.layer.7.intermediate.dense.weight\n",
            "roberta.encoder.layer.7.intermediate.dense.bias\n",
            "roberta.encoder.layer.7.output.dense.weight\n",
            "roberta.encoder.layer.7.output.dense.bias\n",
            "roberta.encoder.layer.7.output.LayerNorm.weight\n",
            "roberta.encoder.layer.7.output.LayerNorm.bias\n",
            "roberta.encoder.layer.8.attention.self.query.weight\n",
            "roberta.encoder.layer.8.attention.self.query.bias\n",
            "roberta.encoder.layer.8.attention.self.key.weight\n",
            "roberta.encoder.layer.8.attention.self.key.bias\n",
            "roberta.encoder.layer.8.attention.self.value.weight\n",
            "roberta.encoder.layer.8.attention.self.value.bias\n",
            "roberta.encoder.layer.8.attention.output.dense.weight\n",
            "roberta.encoder.layer.8.attention.output.dense.bias\n",
            "roberta.encoder.layer.8.attention.output.LayerNorm.weight\n",
            "roberta.encoder.layer.8.attention.output.LayerNorm.bias\n",
            "roberta.encoder.layer.8.intermediate.dense.weight\n",
            "roberta.encoder.layer.8.intermediate.dense.bias\n",
            "roberta.encoder.layer.8.output.dense.weight\n",
            "roberta.encoder.layer.8.output.dense.bias\n",
            "roberta.encoder.layer.8.output.LayerNorm.weight\n",
            "roberta.encoder.layer.8.output.LayerNorm.bias\n",
            "roberta.encoder.layer.9.attention.self.query.weight\n",
            "roberta.encoder.layer.9.attention.self.query.bias\n",
            "roberta.encoder.layer.9.attention.self.key.weight\n",
            "roberta.encoder.layer.9.attention.self.key.bias\n",
            "roberta.encoder.layer.9.attention.self.value.weight\n",
            "roberta.encoder.layer.9.attention.self.value.bias\n",
            "roberta.encoder.layer.9.attention.output.dense.weight\n",
            "roberta.encoder.layer.9.attention.output.dense.bias\n",
            "roberta.encoder.layer.9.attention.output.LayerNorm.weight\n",
            "roberta.encoder.layer.9.attention.output.LayerNorm.bias\n",
            "roberta.encoder.layer.9.intermediate.dense.weight\n",
            "roberta.encoder.layer.9.intermediate.dense.bias\n",
            "roberta.encoder.layer.9.output.dense.weight\n",
            "roberta.encoder.layer.9.output.dense.bias\n",
            "roberta.encoder.layer.9.output.LayerNorm.weight\n",
            "roberta.encoder.layer.9.output.LayerNorm.bias\n",
            "roberta.encoder.layer.10.attention.self.query.weight\n",
            "roberta.encoder.layer.10.attention.self.query.bias\n",
            "roberta.encoder.layer.10.attention.self.key.weight\n",
            "roberta.encoder.layer.10.attention.self.key.bias\n",
            "roberta.encoder.layer.10.attention.self.value.weight\n",
            "roberta.encoder.layer.10.attention.self.value.bias\n",
            "roberta.encoder.layer.10.attention.output.dense.weight\n",
            "roberta.encoder.layer.10.attention.output.dense.bias\n",
            "roberta.encoder.layer.10.attention.output.LayerNorm.weight\n",
            "roberta.encoder.layer.10.attention.output.LayerNorm.bias\n",
            "roberta.encoder.layer.10.intermediate.dense.weight\n",
            "roberta.encoder.layer.10.intermediate.dense.bias\n",
            "roberta.encoder.layer.10.output.dense.weight\n",
            "roberta.encoder.layer.10.output.dense.bias\n",
            "roberta.encoder.layer.10.output.LayerNorm.weight\n",
            "roberta.encoder.layer.10.output.LayerNorm.bias\n",
            "roberta.encoder.layer.11.attention.self.query.weight\n",
            "roberta.encoder.layer.11.attention.self.query.bias\n",
            "roberta.encoder.layer.11.attention.self.key.weight\n",
            "roberta.encoder.layer.11.attention.self.key.bias\n",
            "roberta.encoder.layer.11.attention.self.value.weight\n",
            "roberta.encoder.layer.11.attention.self.value.bias\n",
            "roberta.encoder.layer.11.attention.output.dense.weight\n",
            "roberta.encoder.layer.11.attention.output.dense.bias\n",
            "roberta.encoder.layer.11.attention.output.LayerNorm.weight\n",
            "roberta.encoder.layer.11.attention.output.LayerNorm.bias\n",
            "roberta.encoder.layer.11.intermediate.dense.weight\n",
            "roberta.encoder.layer.11.intermediate.dense.bias\n",
            "roberta.encoder.layer.11.output.dense.weight\n",
            "roberta.encoder.layer.11.output.dense.bias\n",
            "roberta.encoder.layer.11.output.LayerNorm.weight\n",
            "roberta.encoder.layer.11.output.LayerNorm.bias\n",
            "roberta.pooler.dense.weight\n",
            "roberta.pooler.dense.bias\n",
            "mlp_rte.0.weight\n",
            "mlp_rte.0.bias\n",
            "mlp_rte.2.weight\n",
            "mlp_rte.2.bias\n",
            "mlp_mrpc.0.weight\n",
            "mlp_mrpc.0.bias\n",
            "mlp_mrpc.2.weight\n",
            "mlp_mrpc.2.bias\n",
            "emb2mu_rte.weight\n",
            "emb2mu_rte.bias\n",
            "emb2std_rte.weight\n",
            "emb2std_rte.bias\n",
            "emb2mu_mrpc.weight\n",
            "emb2mu_mrpc.bias\n",
            "emb2std_mrpc.weight\n",
            "emb2std_mrpc.bias\n",
            "classifier_rte.weight\n",
            "classifier_rte.bias\n",
            "classifier_mrpc.weight\n",
            "classifier_mrpc.bias\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "optimizer_grouped_parameters = [\n",
        "        {\n",
        "            \"params\": [p for n, p in viroberta.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "            \"weight_decay\": 0.0,\n",
        "        },\n",
        "        {\"params\": [p for n, p in viroberta.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
        "]\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=5e-5, eps=1e-8)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0,\n",
        "                                                num_training_steps=len(train_dataloader)*10)"
      ],
      "metadata": {
        "id": "Out0bXYQn3F0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning) "
      ],
      "metadata": {
        "id": "OOaKQc6AoIMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "transformers.logging.set_verbosity_error()"
      ],
      "metadata": {
        "id": "BNVFNDQyoaXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    #if args.n_gpu > 0:\n",
        "    #    torch.cuda.manual_seed_all(args.seed)\n",
        "    torch.cuda.manual_seed(seed)"
      ],
      "metadata": {
        "id": "mdvAG4fuoiAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score,f1_score\n",
        "\n",
        "n_epochs = 5 #10\n",
        "device=\"cuda\"\n",
        "#vibert.train()\n",
        "viroberta.to(device)\n",
        "viroberta.zero_grad()\n",
        "set_seed(2022)\n",
        "train_loss_rte=[]\n",
        "train_loss_mrpc=[]\n",
        "dev_loss_rte=[]\n",
        "dev_loss_mrpc=[]\n",
        "train_acc_rte=[]\n",
        "train_acc_mrpc=[]\n",
        "dev_acc_rte=[]\n",
        "dev_acc_mrpc=[]\n",
        "total_loss=[]\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  total_train_loss_rte = 0\n",
        "  total_train_loss_mrpc = 0\n",
        "  total_dev_loss_rte = 0\n",
        "  total_dev_loss_mrpc = 0\n",
        "  total_train_acc_rte = 0\n",
        "  total_train_acc_mrpc = 0\n",
        "  total_dev_acc_rte = 0\n",
        "  total_dev_acc_mrpc = 0\n",
        "  total_train_loss=0\n",
        "  vibert.train()\n",
        "  c1_train=0\n",
        "  c2_train=0\n",
        "  c=0\n",
        "  logs=None\n",
        "  labs = None\n",
        "  sampled_labs = None\n",
        "  kl_loss= None\n",
        "  for labels,inp1,inp2,st in tqdm(train_dataloader):\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    batch = tokenizer(text=inp1,text_pair=inp2,max_length=128,truncation=True,padding=True,add_special_tokens=True,is_split_into_words=False,return_tensors='pt')\n",
        "    batch.to(device)\n",
        "\n",
        "    #with torch.set_grad_enabled(True):\n",
        "    out = viroberta(batch[\"input_ids\"],token_type_ids=None, \n",
        "                            attention_mask=batch[\"attention_mask\"],labels=labels.to(device),dataset_name=st[0],logits=logs,label=labs,sampled_logits=sampled_labs,kl_loss=kl_loss)\n",
        "    \n",
        "    #loss = (loss*0.5)/2 \n",
        "    logs= out[\"logits\"].to(device)\n",
        "    labs=labels.to(device)\n",
        "    sampled_labs = out[\"sampled_logits\"].to(device)\n",
        "    kl_loss = out[\"kl_loss\"]\n",
        "    f=0\n",
        "    if st[0]==\"rte\":\n",
        "      c1_train+=1\n",
        "    if st[0]==\"mrpc\":\n",
        "      c2_train+=1\n",
        "    \n",
        "    if c1_train == c2_train:\n",
        "      loss = out[\"loss\"][\"loss\"]  \n",
        "      total_train_loss+=loss.item()\n",
        "      c+=1\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      scheduler.step()  # Update learning rate schedule\n",
        "      viroberta.zero_grad()\n",
        "      logs=None\n",
        "      labs = None\n",
        "      sampled_labs=None\n",
        "      kl_loss = None\n",
        "      \n",
        "      f=1\n",
        "      torch.nn.utils.clip_grad_norm_(vibert.parameters(), 1.0)\n",
        "    if st[0]==\"rte\":\n",
        "        out = viroberta(batch[\"input_ids\"],token_type_ids=None, \n",
        "                            attention_mask=batch[\"attention_mask\"],labels=labels.to(device),dataset_name=st[0],logits=None,label=None,sampled_logits=None,kl_loss=None)\n",
        "        loss = out[\"loss\"][\"loss\"]\n",
        "        total_train_loss_rte += loss.item()\n",
        "        total_train_acc_rte += accuracy_score(labels.cpu(),torch.argmax(out[\"logits\"],1).cpu())  \n",
        "    if st[0]==\"mrpc\":\n",
        "      #total_train_loss_mrpc += loss.item()\n",
        "      out = viroberta(batch[\"input_ids\"],token_type_ids=None, \n",
        "                            attention_mask=batch[\"attention_mask\"],labels=labels.to(device),dataset_name=st[0],logits=None,label=None,sampled_logits=None,kl_loss=None)\n",
        "      loss = out[\"loss\"][\"loss\"]\n",
        "      total_train_loss_mrpc += loss.item()\n",
        "      total_train_acc_mrpc += accuracy_score(labels.cpu(),torch.argmax(out[\"logits\"],1).cpu())\n",
        "    #if f==0 and c1_train+c2_train>=len(train_dataloader):\n",
        "    #  optimizer.step()\n",
        "    #  scheduler.step()  # Update learning rate schedule\n",
        "    #  vibert.zero_grad()\n",
        "  print(\"Epoch \"+str(epoch)+\" RTE Train Loss: \"+str(total_train_loss/len(train_dataloader)))\n",
        "  print(\"Epoch \"+str(epoch)+\" RTE Train Loss: \"+str(total_train_loss_rte/c1_train)+\" MRPC Train Loss: \"+str(total_train_loss_mrpc/c2_train))\n",
        "  print(\" RTE Train Acc: \"+str(total_train_acc_rte/c1_train)+\" MRPC Train Acc: \"+str(total_train_acc_mrpc/c2_train))\n",
        "  total_loss.append(total_train_loss/c)\n",
        "  train_loss_rte.append(total_train_loss_rte/c1_train)\n",
        "  train_loss_mrpc.append(total_train_loss_mrpc/c2_train)\n",
        "  train_acc_rte.append(total_train_acc_rte/c1_train)\n",
        "  train_acc_mrpc.append(total_train_acc_mrpc/c2_train)\n",
        "  \n",
        "\n",
        "  \n",
        "  vibert.eval()\n",
        "  c1_dev=0\n",
        "  c2_dev=0\n",
        "  for labels,inp1,inp2,st in tqdm(dev_dataloader):\n",
        "    \n",
        "    batch = tokenizer(text=inp1,text_pair=inp2,max_length=128,truncation=True,padding=True,is_split_into_words=False,return_tensors='pt')\n",
        "    batch.to(device)\n",
        "    \n",
        "    out = viroberta(batch[\"input_ids\"],token_type_ids=None, \n",
        "                             attention_mask=batch[\"attention_mask\"],labels=labels.to(device),dataset_name=st[0],logits=None,label=None,sampled_logits=None,kl_loss=None)\n",
        "    loss = out[\"loss\"][\"loss\"]\n",
        "    if st[0]==\"rte\":\n",
        "      total_dev_loss_rte += loss.item()\n",
        "      total_dev_acc_rte += accuracy_score(labels.cpu(),torch.argmax(out[\"logits\"],1).cpu())\n",
        "      c1_dev+=1\n",
        "    if st[0]==\"mrpc\":\n",
        "      total_dev_loss_mrpc += loss.item()\n",
        "      total_dev_acc_mrpc += accuracy_score(labels.cpu(),torch.argmax(out[\"logits\"],1).cpu())\n",
        "      c2_dev+=1\n",
        "     \n",
        "  print(\"Epoch \"+str(epoch)+\" RTE Val Loss: \"+str(total_dev_loss_rte/c1_dev)+\" MRPC Val Loss: \"+str(total_dev_loss_mrpc/c2_dev))\n",
        "  print(\" RTE Val Acc: \"+str(total_dev_acc_rte/c1_dev)+\" MRPC Val Acc: \"+str(total_dev_acc_mrpc/c2_dev))\n",
        "  \n",
        "  dev_loss_rte.append(total_dev_loss_rte/c1_dev)\n",
        "  dev_loss_mrpc.append(total_dev_loss_mrpc/c2_dev)\n",
        "  dev_acc_rte.append(total_dev_acc_rte/c1_dev)\n",
        "  dev_acc_mrpc.append(total_dev_acc_mrpc/c2_dev)\n",
        "  torch.save(vibert.state_dict(), \"vibert_70\")\n",
        "\n",
        "\n",
        "print(\"Total_train_loss : \",total_loss)\n",
        "print(\"RTE Train Loss: \",train_loss_rte)\n",
        "print(\"MRPC Train Loss: \",train_loss_mrpc)\n",
        "print(\"RTE Dev Loss: \",dev_loss_rte)\n",
        "print(\"MRPC Dev Loss: \",dev_loss_mrpc)\n",
        "print(\"RTE Train Acc: \",train_acc_rte)\n",
        "print(\"MRPC Train Acc: \",train_acc_mrpc)\n",
        "print(\"RTE Dev Acc: \",dev_acc_rte)\n",
        "print(\"MRPC Dev Acc: \",dev_acc_mrpc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2OOBiOootpn",
        "outputId": "e98e2e3e-f6a9-4bd8-9439-68c3c6ff1d56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [02:19<00:00,  1.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 RTE Train Loss: 0.7387236894345751\n",
            "Epoch 0 RTE Train Loss: 0.8145382036181057 MRPC Train Loss: 0.6192540765977373\n",
            " RTE Train Acc: 0.4993872549019608 MRPC Train Acc: 0.7300857843137255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:09<00:00,  5.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 RTE Val Loss: 0.7256500377104833 MRPC Val Loss: 0.4192550308429278\n",
            " RTE Val Acc: 0.49399038461538464 MRPC Val Acc: 0.8401442307692307\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [02:25<00:00,  1.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 RTE Train Loss: 0.5642307435764986\n",
            "Epoch 1 RTE Train Loss: 0.7218116030973547 MRPC Train Loss: 0.3347247585508169\n",
            " RTE Train Acc: 0.5033700980392157 MRPC Train Acc: 0.8762254901960784\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:09<00:00,  5.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 RTE Val Loss: 0.7109171610612136 MRPC Val Loss: 0.3707012918133002\n",
            " RTE Val Acc: 0.5252403846153846 MRPC Val Acc: 0.8449519230769231\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [02:26<00:00,  1.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 RTE Train Loss: 0.47150760158604266\n",
            "Epoch 2 RTE Train Loss: 0.7026020492993149 MRPC Train Loss: 0.16982318950342198\n",
            " RTE Train Acc: 0.5471813725490197 MRPC Train Acc: 0.9549632352941176\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:10<00:00,  4.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 RTE Val Loss: 0.6883068336890295 MRPC Val Loss: 0.33044513028401595\n",
            " RTE Val Acc: 0.5769230769230769 MRPC Val Acc: 0.8617788461538461\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [02:26<00:00,  1.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 RTE Train Loss: 0.3496816056031807\n",
            "Epoch 3 RTE Train Loss: 0.5716955243956809 MRPC Train Loss: 0.0780253768442016\n",
            " RTE Train Acc: 0.7282475490196079 MRPC Train Acc: 0.9859068627450981\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:09<00:00,  5.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 RTE Val Loss: 0.7690158967788403 MRPC Val Loss: 0.3557808525287188\n",
            " RTE Val Acc: 0.6274038461538461 MRPC Val Acc: 0.8617788461538461\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [02:26<00:00,  1.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 RTE Train Loss: 0.19933091421775959\n",
            "Epoch 4 RTE Train Loss: 0.3175708353884664 MRPC Train Loss: 0.04954649093469568\n",
            " RTE Train Acc: 0.8857230392156863 MRPC Train Acc: 0.9938725490196079\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:09<00:00,  5.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 RTE Val Loss: 0.8695118610675519 MRPC Val Loss: 0.43847251855410063\n",
            " RTE Val Acc: 0.6923076923076923 MRPC Val Acc: 0.8701923076923077\n",
            "Total_train_loss :  [1.4774473788691502, 1.1284614871529972, 0.9430152031720853, 0.6993632112063614, 0.39866182843551917]\n",
            "RTE Train Loss:  [0.8145382036181057, 0.7218116030973547, 0.7026020492993149, 0.5716955243956809, 0.3175708353884664]\n",
            "MRPC Train Loss:  [0.6192540765977373, 0.3347247585508169, 0.16982318950342198, 0.0780253768442016, 0.04954649093469568]\n",
            "RTE Dev Loss:  [0.7256500377104833, 0.7109171610612136, 0.6883068336890295, 0.7690158967788403, 0.8695118610675519]\n",
            "MRPC Dev Loss:  [0.4192550308429278, 0.3707012918133002, 0.33044513028401595, 0.3557808525287188, 0.43847251855410063]\n",
            "RTE Train Acc:  [0.4993872549019608, 0.5033700980392157, 0.5471813725490197, 0.7282475490196079, 0.8857230392156863]\n",
            "MRPC Train Acc:  [0.7300857843137255, 0.8762254901960784, 0.9549632352941176, 0.9859068627450981, 0.9938725490196079]\n",
            "RTE Dev Acc:  [0.49399038461538464, 0.5252403846153846, 0.5769230769230769, 0.6274038461538461, 0.6923076923076923]\n",
            "MRPC Dev Acc:  [0.8401442307692307, 0.8449519230769231, 0.8617788461538461, 0.8617788461538461, 0.8701923076923077]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "device=\"cuda\"\n",
        "#vibert.train()\n",
        "viroberta.to(device)\n",
        "set_seed(2022)\n",
        "vibert.eval()\n",
        "total_test_loss_rte = 0\n",
        "total_test_loss_mrpc = 0\n",
        "total_test_acc_rte = 0\n",
        "total_test_acc_mrpc = 0\n",
        "total_test_f1_rte = 0\n",
        "total_test_f1_mrpc = 0\n",
        "c1=1\n",
        "c2=0\n",
        "epoch=1\n",
        "for labels,inp1,inp2,st in tqdm(mrpc_test_dataloader):\n",
        "  \n",
        "  batch = tokenizer(text=inp1,text_pair=inp2,max_length=128,truncation=True,padding=True,is_split_into_words=False,return_tensors='pt')\n",
        "  batch.to(device)\n",
        "  out = viroberta(batch[\"input_ids\"],token_type_ids=None, \n",
        "                             attention_mask=batch[\"attention_mask\"],labels=labels.to(device),dataset_name=st[0],logits=None,label=None,sampled_logits=None,kl_loss=None)\n",
        "  loss = out[\"loss\"][\"loss\"]\n",
        "  if st[0]==\"rte\":\n",
        "    total_test_loss_rte += loss.item()\n",
        "    total_test_acc_rte += accuracy_score(labels.cpu(),torch.argmax(out[\"logits\"],1).cpu())\n",
        "    total_test_f1_rte += f1_score(labels.cpu(),torch.argmax(out[\"logits\"],1).cpu())\n",
        "    c1+=1\n",
        "  if st[0]==\"mrpc\":\n",
        "    total_test_loss_mrpc += loss.item()\n",
        "    total_test_acc_mrpc += accuracy_score(labels.cpu(),torch.argmax(out[\"logits\"],1).cpu())\n",
        "    total_test_f1_mrpc += f1_score(labels.cpu(),torch.argmax(out[\"logits\"],1).cpu())\n",
        "    c2+=1\n",
        "    \n",
        "print(\"Epoch \"+str(epoch)+\" RTE Test Loss: \"+str(total_test_loss_rte/c1)+\" MRPC Test Loss: \"+str(total_test_loss_mrpc/c2))\n",
        "print(\" RTE Test Acc: \"+str(total_test_acc_rte/c1)+\" MRPC Test Acc: \"+str(total_test_acc_mrpc/c2))\n",
        "print(\" RTE Test F1: \"+str(total_test_f1_rte/c1)+\" MRPC Test F1: \"+str(total_test_f1_mrpc/c2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlIHI13ItR-T",
        "outputId": "858ff339-96ab-4fd2-fc7f-499475570f07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 54/54 [00:10<00:00,  5.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 RTE Test Loss: 0.0 MRPC Test Loss: 0.4533645397535077\n",
            " RTE Test Acc: 0.0 MRPC Test Acc: 0.8557830459770115\n",
            " RTE Test F1: 0.0 MRPC Test F1: 0.8854379131077169\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Shared Encoder beta 1e-04\n",
        "\n",
        "Total_train_loss :  [1.4752381292043948, 1.0763221412312751, 0.7535858379275191, 0.4824579087834732, 0.26287615898193095]\n",
        "RTE Train Loss:  [0.7970023856443518, 0.6611801941020816, 0.5197818195995163, 0.353616383291927, 0.19068483595608496]\n",
        "MRPC Train Loss:  [0.6353617950397379, 0.32936984592793034, 0.1635296375070717, 0.08220969421752528, 0.050292712070193944]\n",
        "RTE Dev Loss:  [0.7252618257816021, 0.6153905345843389, 0.6958389351001153, 0.7472880757772006, 0.9052459689287039]\n",
        "MRPC Dev Loss:  [0.5984869473255597, 0.3884557227675731, 0.35589291098026127, 0.4119063802063465, 0.5404707909776614]\n",
        "RTE Train Acc:  [0.5217524509803921, 0.6314338235294118, 0.7662377450980392, 0.8700980392156863, 0.9463848039215687]\n",
        "MRPC Train Acc:  [0.6936274509803921, 0.8939950980392157, 0.9613970588235294, 0.9871323529411765, 0.9941789215686274]\n",
        "RTE Dev Acc:  [0.5396634615384616, 0.7199519230769231, 0.6814903846153846, 0.671875, 0.7271634615384616]\n",
        "MRPC Dev Acc:  [0.6826923076923077, 0.8461538461538461, 0.8497596153846154, 0.859375, 0.8533653846153846]\n",
        "\n",
        "Epoch 1 RTE Test Loss: 0.0 MRPC Test Loss: 0.5804826705544083\n",
        " RTE Test Acc: 0.0 MRPC Test Acc: 0.8517321200510856\n",
        " RTE Test F1: 0.0 MRPC Test F1: 0.8872017898455994\n",
        "\n",
        "Epoch 1 RTE Test Loss: 0.9051229586968055 MRPC Test Loss: 0.543636390222953\n",
        " RTE Test Acc: 0.7259615384615384 MRPC Test Acc: 0.8497596153846154\n",
        " RTE Test F1: 0.6913012994273888 MRPC Test F1: 0.8883961952858421\n",
        "\n",
        "\n",
        "Task Specific Encoder beta 1e-04\n",
        "\n",
        "Total_train_loss :  [1.4774473788691502, 1.1284614871529972, 0.9430152031720853, 0.6993632112063614, 0.39866182843551917]\n",
        "RTE Train Loss:  [0.8145382036181057, 0.7218116030973547, 0.7026020492993149, 0.5716955243956809, 0.3175708353884664]\n",
        "MRPC Train Loss:  [0.6192540765977373, 0.3347247585508169, 0.16982318950342198, 0.0780253768442016, 0.04954649093469568]\n",
        "RTE Dev Loss:  [0.7256500377104833, 0.7109171610612136, 0.6883068336890295, 0.7690158967788403, 0.8695118610675519]\n",
        "MRPC Dev Loss:  [0.4192550308429278, 0.3707012918133002, 0.33044513028401595, 0.3557808525287188, 0.43847251855410063]\n",
        "RTE Train Acc:  [0.4993872549019608, 0.5033700980392157, 0.5471813725490197, 0.7282475490196079, 0.8857230392156863]\n",
        "MRPC Train Acc:  [0.7300857843137255, 0.8762254901960784, 0.9549632352941176, 0.9859068627450981, 0.9938725490196079]\n",
        "RTE Dev Acc:  [0.49399038461538464, 0.5252403846153846, 0.5769230769230769, 0.6274038461538461, 0.6923076923076923]\n",
        "MRPC Dev Acc:  [0.8401442307692307, 0.8449519230769231, 0.8617788461538461, 0.8617788461538461, 0.8701923076923077]\n",
        "\n",
        "Epoch 1 RTE Test Loss: 0.0 MRPC Test Loss: 0.4533645397535077\n",
        " RTE Test Acc: 0.0 MRPC Test Acc: 0.8557830459770115\n",
        " RTE Test F1: 0.0 MRPC Test F1: 0.8854379131077169"
      ],
      "metadata": {
        "id": "8mLggJYctfBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Froze the BERT Model(Only using the last layer)"
      ],
      "metadata": {
        "id": "ffsy1cKaKA4M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "config = BertConfig.from_pretrained(\n",
        "        \"bert-base-uncased\",\n",
        "        num_labels=2)\n",
        "vibert_froze = BertForSequenceClassification.from_pretrained(\n",
        "        \"bert-base-uncased\",\n",
        "        config=config\n",
        ")"
      ],
      "metadata": {
        "id": "ueidO-HAAgA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name,param in vibert_froze.bert.named_parameters():\n",
        "    #print(name,\"encoder.layer.11\" not in name)\n",
        "    if \"encoder.layer.11\" not in name:\n",
        "      param.requires_grad = False\n",
        "    else:\n",
        "      break"
      ],
      "metadata": {
        "id": "h00Lhb3dKUjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in vibert_froze.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYSwol7NKjRU",
        "outputId": "e7733ccb-45a5-4cea-92fe-a222ea319c1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mu_p\n",
            "std_p\n",
            "bert.encoder.layer.11.attention.self.query.weight\n",
            "bert.encoder.layer.11.attention.self.query.bias\n",
            "bert.encoder.layer.11.attention.self.key.weight\n",
            "bert.encoder.layer.11.attention.self.key.bias\n",
            "bert.encoder.layer.11.attention.self.value.weight\n",
            "bert.encoder.layer.11.attention.self.value.bias\n",
            "bert.encoder.layer.11.attention.output.dense.weight\n",
            "bert.encoder.layer.11.attention.output.dense.bias\n",
            "bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
            "bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
            "bert.encoder.layer.11.intermediate.dense.weight\n",
            "bert.encoder.layer.11.intermediate.dense.bias\n",
            "bert.encoder.layer.11.output.dense.weight\n",
            "bert.encoder.layer.11.output.dense.bias\n",
            "bert.encoder.layer.11.output.LayerNorm.weight\n",
            "bert.encoder.layer.11.output.LayerNorm.bias\n",
            "bert.pooler.dense.weight\n",
            "bert.pooler.dense.bias\n",
            "mlp.0.weight\n",
            "mlp.0.bias\n",
            "mlp.2.weight\n",
            "mlp.2.bias\n",
            "emb2mu.weight\n",
            "emb2mu.bias\n",
            "emb2std.weight\n",
            "emb2std.bias\n",
            "classifier_rte.weight\n",
            "classifier_rte.bias\n",
            "classifier_mrpc.weight\n",
            "classifier_mrpc.bias\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "optimizer_grouped_parameters = [\n",
        "        {\n",
        "            \"params\": [p for n, p in vibert_froze.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "            \"weight_decay\": 0.0,\n",
        "        },\n",
        "        {\"params\": [p for n, p in vibert_froze.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
        "]\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=5e-5, eps=1e-8)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0,\n",
        "                                                num_training_steps=len(train_dataloader)*10)"
      ],
      "metadata": {
        "id": "mcZJRL7iKvMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning) "
      ],
      "metadata": {
        "id": "YiHJiYoALTtG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "transformers.logging.set_verbosity_error()"
      ],
      "metadata": {
        "id": "KqOT-r5WLftk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    #if args.n_gpu > 0:\n",
        "    #    torch.cuda.manual_seed_all(args.seed)\n",
        "    torch.cuda.manual_seed(seed)"
      ],
      "metadata": {
        "id": "qkkmXh9RLioc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score,f1_score\n",
        "\n",
        "n_epochs = 10\n",
        "device=\"cuda\"\n",
        "#vibert.train()\n",
        "vibert_froze.to(device)\n",
        "vibert_froze.zero_grad()\n",
        "set_seed(2022)\n",
        "for epoch in range(n_epochs):\n",
        "  total_train_loss_rte = 0\n",
        "  total_train_loss_mrpc = 0\n",
        "  total_dev_loss_rte = 0\n",
        "  total_dev_loss_mrpc = 0\n",
        "  total_train_acc_rte = 0\n",
        "  total_train_acc_mrpc = 0\n",
        "  total_dev_acc_rte = 0\n",
        "  total_dev_acc_mrpc = 0\n",
        "  vibert_froze.train()\n",
        "  c1_train=0\n",
        "  c2_train=0\n",
        "  for labels,inp1,inp2,st in tqdm(train_dataloader):\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    batch = tokenizer(text=inp1,text_pair=inp2,max_length=128,truncation=True,padding=True,add_special_tokens=True,is_split_into_words=False,return_tensors='pt')\n",
        "    batch.to(device)\n",
        "  \n",
        "    out = vibert_froze(batch[\"input_ids\"],token_type_ids=None, \n",
        "                             attention_mask=batch[\"attention_mask\"],labels=labels.to(device),dataset_name=st[0])\n",
        "    loss = out[\"loss\"][\"loss\"]\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()  # Update learning rate schedule\n",
        "    vibert_froze.zero_grad()\n",
        "    \n",
        "    if st[0]==\"rte\":\n",
        "      total_train_loss_rte += loss.item()\n",
        "      total_train_acc_rte += accuracy_score(labels.cpu(),torch.argmax(out[\"logits\"],1).cpu())  \n",
        "      c1_train+=1\n",
        "    if st[0]==\"mrpc\":\n",
        "      total_train_loss_mrpc += loss.item()\n",
        "      total_train_acc_mrpc += accuracy_score(labels.cpu(),torch.argmax(out[\"logits\"],1).cpu())\n",
        "      c2_train+=1\n",
        "    torch.nn.utils.clip_grad_norm_(vibert_froze.parameters(), 1.0)\n",
        "  \n",
        "  print(\"Epoch \"+str(epoch)+\" RTE Train Loss: \"+str(total_train_loss_rte/c1_train)+\" MRPC Train Loss: \"+str(total_train_loss_mrpc/c2_train))\n",
        "  print(\" RTE Train Acc: \"+str(total_train_acc_rte/c1_train)+\" MRPC Train Acc: \"+str(total_train_acc_mrpc/c2_train))\n",
        "  \n",
        "  vibert.eval()\n",
        "  c1_dev=0\n",
        "  c2_dev=0\n",
        "  for labels,inp1,inp2,st in tqdm(dev_dataloader):\n",
        "    \n",
        "    batch = tokenizer(text=inp1,text_pair=inp2,max_length=128,truncation=True,padding=True,is_split_into_words=False,return_tensors='pt')\n",
        "    batch.to(device)\n",
        "    \n",
        "    out = vibert_froze(batch[\"input_ids\"],token_type_ids=None, \n",
        "                             attention_mask=batch[\"attention_mask\"],labels=labels.to(device),dataset_name=st[0])\n",
        "    loss = out[\"loss\"][\"loss\"]\n",
        "    if st[0]==\"rte\":\n",
        "      total_dev_loss_rte += loss.item()\n",
        "      total_dev_acc_rte += accuracy_score(labels.cpu(),torch.argmax(out[\"logits\"],1).cpu())\n",
        "      c1_dev+=1\n",
        "    if st[0]==\"mrpc\":\n",
        "      total_dev_loss_mrpc += loss.item()\n",
        "      total_dev_acc_mrpc += accuracy_score(labels.cpu(),torch.argmax(out[\"logits\"],1).cpu())\n",
        "      c2_dev+=1\n",
        "     \n",
        "  print(\"Epoch \"+str(epoch)+\" RTE Val Loss: \"+str(total_dev_loss_rte/c1_dev)+\" MRPC Val Loss: \"+str(total_dev_loss_mrpc/c2_dev))\n",
        "  print(\" RTE Val Acc: \"+str(total_dev_acc_rte/c1_dev)+\" MRPC Val Acc: \"+str(total_dev_acc_mrpc/c2_dev))\n",
        "  \n",
        "  torch.save(vibert_froze.state_dict(), \"vibert_froze_10\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjKYtzJXLlyO",
        "outputId": "034b09ef-9a54-40d6-b381-02ddc33c3b29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [01:05<00:00,  3.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 RTE Train Loss: 0.7259384814430686 MRPC Train Loss: 0.643480969118137\n",
            " RTE Train Acc: 0.5076593137254902 MRPC Train Acc: 0.6712622549019608\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:12<00:00,  4.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 RTE Val Loss: 0.7114817729363074 MRPC Val Loss: 0.576663178893236\n",
            " RTE Val Acc: 0.5024038461538461 MRPC Val Acc: 0.7223557692307693\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [00:57<00:00,  3.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 RTE Train Loss: 0.7056536686186697 MRPC Train Loss: 0.5579975688574361\n",
            " RTE Train Acc: 0.53125 MRPC Train Acc: 0.71875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:12<00:00,  4.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 RTE Val Loss: 0.7133175478531764 MRPC Val Loss: 0.56043940782547\n",
            " RTE Val Acc: 0.4987980769230769 MRPC Val Acc: 0.7283653846153846\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [00:57<00:00,  3.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 RTE Train Loss: 0.6828363859185985 MRPC Train Loss: 0.5146735482940487\n",
            " RTE Train Acc: 0.585171568627451 MRPC Train Acc: 0.7515318627450981\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:12<00:00,  4.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 RTE Val Loss: 0.6922160547513229 MRPC Val Loss: 0.5562845571683004\n",
            " RTE Val Acc: 0.5444711538461539 MRPC Val Acc: 0.71875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [00:57<00:00,  3.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 RTE Train Loss: 0.6442102386670954 MRPC Train Loss: 0.4619426258346614\n",
            " RTE Train Acc: 0.6412377450980392 MRPC Train Acc: 0.7867647058823529\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:12<00:00,  4.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 RTE Val Loss: 0.716391038436156 MRPC Val Loss: 0.5591597087108172\n",
            " RTE Val Acc: 0.5612980769230769 MRPC Val Acc: 0.7235576923076923\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [00:56<00:00,  3.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 RTE Train Loss: 0.5879380077708001 MRPC Train Loss: 0.3853791323946972\n",
            " RTE Train Acc: 0.703125 MRPC Train Acc: 0.8434436274509803\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:12<00:00,  4.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 RTE Val Loss: 0.778529343696741 MRPC Val Loss: 0.6701420918107033\n",
            " RTE Val Acc: 0.5661057692307693 MRPC Val Acc: 0.7307692307692307\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [00:57<00:00,  3.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 RTE Train Loss: 0.5151910843218074 MRPC Train Loss: 0.33074869200879453\n",
            " RTE Train Acc: 0.7558210784313726 MRPC Train Acc: 0.8648897058823529\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:12<00:00,  4.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 RTE Val Loss: 0.9154392595474536 MRPC Val Loss: 0.6654941198917536\n",
            " RTE Val Acc: 0.5108173076923077 MRPC Val Acc: 0.7103365384615384\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [00:57<00:00,  3.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 RTE Train Loss: 0.4459810275949684 MRPC Train Loss: 0.2617034958858116\n",
            " RTE Train Acc: 0.8023897058823529 MRPC Train Acc: 0.9019607843137255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:12<00:00,  4.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 RTE Val Loss: 0.9663237126973959 MRPC Val Loss: 0.7546284863582025\n",
            " RTE Val Acc: 0.5036057692307693 MRPC Val Acc: 0.7199519230769231\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [00:57<00:00,  3.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 RTE Train Loss: 0.3678647079596333 MRPC Train Loss: 0.22408832719220834\n",
            " RTE Train Acc: 0.8452818627450981 MRPC Train Acc: 0.9108455882352942\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:12<00:00,  4.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 RTE Val Loss: 1.1350070811234987 MRPC Val Loss: 0.8238578748244506\n",
            " RTE Val Acc: 0.5036057692307693 MRPC Val Acc: 0.7271634615384616\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [00:57<00:00,  3.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 RTE Train Loss: 0.31519585760200725 MRPC Train Loss: 0.18394529074430466\n",
            " RTE Train Acc: 0.8697916666666666 MRPC Train Acc: 0.930453431372549\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:12<00:00,  4.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 RTE Val Loss: 1.2248283762198229 MRPC Val Loss: 0.8577820005325171\n",
            " RTE Val Acc: 0.515625 MRPC Val Acc: 0.7079326923076923\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 204/204 [00:59<00:00,  3.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 RTE Train Loss: 0.28206749961656685 MRPC Train Loss: 0.17222489080592698\n",
            " RTE Train Acc: 0.8832720588235294 MRPC Train Acc: 0.9387254901960784\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 52/52 [00:12<00:00,  4.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 RTE Val Loss: 1.2437252471080193 MRPC Val Loss: 0.8951379198294419\n",
            " RTE Val Acc: 0.5012019230769231 MRPC Val Acc: 0.7019230769230769\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "device=\"cuda\"\n",
        "#vibert.train()\n",
        "vibert_froze.to(device)\n",
        "set_seed(2022)\n",
        "vibert_froze.eval()\n",
        "total_test_loss_rte = 0\n",
        "total_test_loss_mrpc = 0\n",
        "total_test_acc_rte = 0\n",
        "total_test_acc_mrpc = 0\n",
        "total_test_f1_rte = 0\n",
        "total_test_f1_mrpc = 0\n",
        "c1=1\n",
        "c2=0\n",
        "epoch=1\n",
        "for labels,inp1,inp2,st in tqdm(mrpc_test_dataloader):\n",
        "  \n",
        "  batch = tokenizer(text=inp1,text_pair=inp2,max_length=128,truncation=True,padding=True,is_split_into_words=False,return_tensors='pt')\n",
        "  batch.to(device)\n",
        "  out = vibert_froze(batch[\"input_ids\"],token_type_ids=None, \n",
        "                             attention_mask=batch[\"attention_mask\"],labels=labels.to(device),dataset_name=st[0])\n",
        "  loss = out[\"loss\"][\"loss\"]\n",
        "  if st[0]==\"rte\":\n",
        "    total_test_loss_rte += loss.item()\n",
        "    total_test_acc_rte += accuracy_score(labels.cpu(),torch.argmax(out[\"logits\"],1).cpu())\n",
        "    total_test_f1_rte += f1_score(labels.cpu(),torch.argmax(out[\"logits\"],1).cpu())\n",
        "    c1+=1\n",
        "  if st[0]==\"mrpc\":\n",
        "    total_test_loss_mrpc += loss.item()\n",
        "    total_test_acc_mrpc += accuracy_score(labels.cpu(),torch.argmax(out[\"logits\"],1).cpu())\n",
        "    total_test_f1_mrpc += f1_score(labels.cpu(),torch.argmax(out[\"logits\"],1).cpu())\n",
        "    c2+=1\n",
        "    \n",
        "print(\"Epoch \"+str(epoch)+\" RTE Test Loss: \"+str(total_test_loss_rte/c1)+\" MRPC Test Loss: \"+str(total_test_loss_mrpc/c2))\n",
        "print(\" RTE Test Acc: \"+str(total_test_acc_rte/c1)+\" MRPC Test Acc: \"+str(total_test_acc_mrpc/c2))\n",
        "print(\" RTE Test F1: \"+str(total_test_f1_rte/c1)+\" MRPC Test F1: \"+str(total_test_f1_mrpc/c2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vuAC9WfME_g",
        "outputId": "bfb4397d-215e-44bc-8039-2fe04a423821"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 54/54 [00:12<00:00,  4.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 RTE Test Loss: 0.0 MRPC Test Loss: 0.9202623113437935\n",
            " RTE Test Acc: 0.0 MRPC Test Acc: 0.7088322158365261\n",
            " RTE Test F1: 0.0 MRPC Test F1: 0.7922380277319566\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8DuBzmbjPBef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8zELxLCtdI26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NnmsVjHEdL4a"
      }
    }
  ]
}